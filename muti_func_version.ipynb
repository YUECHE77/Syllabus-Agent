{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "F_eE_DNsARvt"
      },
      "outputs": [],
      "source": [
        "!pip install -q together\n",
        "!pip install -q FlagEmbedding\n",
        "!pip install -q peft\n",
        "!pip install -q faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocRbhpTZ3L3z"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "TOGETHER_API_KEY = userdata.get('TOGETHER_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14UzEZT8-8L2",
        "outputId": "a64aa426-7116-4705-a0d3-9d19147e3366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMd5gnmYlWmp"
      },
      "outputs": [],
      "source": [
        "from FlagEmbedding import FlagModel\n",
        "from FlagEmbedding import FlagReranker\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import faiss\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "from together import Together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bh5PlmHxkPyr"
      },
      "outputs": [],
      "source": [
        "together = Together(api_key=TOGETHER_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "d1SGcTmOYcCi"
      },
      "outputs": [],
      "source": [
        "# encode_queries()和encode()的区别：\n",
        "# for s2p (short query to long passage) retrieval task, suggest to use encode_queries() which will automatically add the instruction to each query\n",
        "# corpus in retrieval task can still use encode() or encode_corpus(), since they don't need instruction\n",
        "\n",
        "model = FlagModel('BAAI/bge-base-en-v1.5', query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\", use_fp16=True)\n",
        "# print(model.device)\n",
        "\n",
        "reranker = FlagReranker('BAAI/bge-reranker-base', use_fp16=True, device='cuda')\n",
        "# print(reranker.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgg1y2v2VtZ6"
      },
      "outputs": [],
      "source": [
        "# query = \"Tell me the exam information for the database class.\"\n",
        "query = \"what is happening in Los Angeles?\"\n",
        "\n",
        "courses_numbers = ['544', '566', '585', '596', '599', '626', '677', '699']\n",
        "course_names = ['Applied Natural Language Processing (NLP)', 'Deep Learning and its Applications (DL)', 'Database Systems (database)', 'Scientific Computing and Visualization',\n",
        "         'Distributed Systems', 'Text as Data', 'Advanced Computer Vision (CV)', 'Robotic Perception (Robotics)']\n",
        "\n",
        "full_course_info = dict(zip(['CSCI' + num for num in courses_numbers], ['CSCI' + num + ' ' + name for num, name in zip(courses_numbers, course_names)]))\n",
        "\n",
        "name_to_num = dict(zip(course_names, courses_numbers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcfesIpb24G1",
        "outputId": "12083460-15a2-47d3-cd75-0619b844a0cc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'CSCI544': 'CSCI544 Applied Natural Language Processing (NLP)',\n",
              " 'CSCI566': 'CSCI566 Deep Learning and its Applications (DL)',\n",
              " 'CSCI585': 'CSCI585 Database Systems (database)',\n",
              " 'CSCI596': 'CSCI596 Scientific Computing and Visualization',\n",
              " 'CSCI599': 'CSCI599 Distributed Systems',\n",
              " 'CSCI626': 'CSCI626 Text as Data',\n",
              " 'CSCI677': 'CSCI677 Advanced Computer Vision (CV)',\n",
              " 'CSCI699': 'CSCI699 Robotic Perception (Robotics)'}"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ],
      "source": [
        "full_course_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzdFZ3LiZIji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2d2491f-2ae9-4814-8040-d6d4b629ca70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        }
      ],
      "source": [
        "def build_vector_database(course_names):\n",
        "    course_embeddings = model.encode(course_names)\n",
        "    np_course_embeddings = np.array(course_embeddings).astype('float32')\n",
        "\n",
        "    # with normalization -> bacome cosine similarity search\n",
        "    faiss.normalize_L2(np_course_embeddings)\n",
        "\n",
        "    # IndexFlatIP: dot-product search, IndexFlatL2: L2 search\n",
        "    index_innerproduct = faiss.IndexFlatIP(len(course_embeddings[0]))\n",
        "    res = faiss.StandardGpuResources()  # Create GPU resources\n",
        "    gpu_index = faiss.index_cpu_to_gpu(res, 0, index_innerproduct) # Move index to GPU\n",
        "\n",
        "    gpu_index.add(np_course_embeddings)\n",
        "\n",
        "    return gpu_index\n",
        "\n",
        "database = build_vector_database(course_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcvkLGk4ZbFi"
      },
      "outputs": [],
      "source": [
        "def search_(query, database, embed_dim, topk=1):\n",
        "    query_embed = model.encode_queries(query)\n",
        "    query_embed = np.array(query_embed).astype('float32')\n",
        "\n",
        "    _, idx = database.search(query_embed.reshape((1, embed_dim)), topk) # (768,) -> (1, 768)\n",
        "    idx = idx.reshape(-1)\n",
        "\n",
        "    ret = [course_names[i] for i in idx]\n",
        "\n",
        "    return ret\n",
        "\n",
        "def find_syllabus(query, courses_numbers, course_names, name_to_num, model, database):\n",
        "    def find_course():\n",
        "        for num in courses_numbers:\n",
        "            if num in query:\n",
        "                return num\n",
        "\n",
        "        result = search_(query, database, embed_dim=database.d, topk=1)\n",
        "        return result[0]\n",
        "\n",
        "    result = find_course()\n",
        "\n",
        "    if result in courses_numbers:\n",
        "        return 'CSCI' + result + '.txt'\n",
        "    else:\n",
        "        return 'CSCI' + name_to_num[result] + '.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpc548UOjMQR",
        "outputId": "83e4f871-19bb-45ad-f7d8-d5c376ea727f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSCI596.txt\n"
          ]
        }
      ],
      "source": [
        "print(find_syllabus(query, courses_numbers, course_names, name_to_num, model, database))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEdJxWWpaPm1"
      },
      "outputs": [],
      "source": [
        "# 读取txt文件并按\"\\n\\n\"切割 -> 按照需求自行修改！！！\n",
        "def read_and_split_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    segments = content.split('##')\n",
        "    # print(segments)\n",
        "\n",
        "    return [seg.strip() for seg in segments]  # 变成了一个list\n",
        "\n",
        "# 计算query和每个文档分段的相似度，并返回排序后的结果\n",
        "def compute_similarity(query, segments):\n",
        "    # encode_queries是专门用于encode query的\n",
        "    query_embedding = model.encode_queries(query)  # [512, ]\n",
        "    segment_embeddings = model.encode(segments)  # [num_seg, 512]\n",
        "\n",
        "    # 官方提供的。只计算点积，不计算cos-simi\n",
        "    similarities = [query_embedding @ segment_embedding.T for segment_embedding in segment_embeddings]\n",
        "\n",
        "    # 将分数、query和文档分段组合并排序 -> 降序\n",
        "    results = sorted(zip(similarities, segments), key=lambda x: x[0], reverse=True)\n",
        "    # print(results)\n",
        "\n",
        "    return results\n",
        "\n",
        "# 定义生成查询和文档组合的函数\n",
        "def generate_query_passage_pairs(query, passages):\n",
        "    return [[query[0], passage] for passage in passages]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOAOWd0YV4CE"
      },
      "outputs": [],
      "source": [
        "def RAG(query):\n",
        "\n",
        "    if query is None:\n",
        "        raise ValueError('Your query cannot be empty!')\n",
        "\n",
        "    k = 3\n",
        "\n",
        "    final_syllabus = find_syllabus(query, courses_numbers, course_names, name_to_num, model, database)\n",
        "    print(final_syllabus)\n",
        "\n",
        "    # file_path = '/content' + os.sep + final_syllabus\n",
        "    file_path = '/content/drive/MyDrive/CSCI544/Project/syllabus/' + final_syllabus\n",
        "    knowledge_base = read_and_split_file(file_path)\n",
        "\n",
        "    query = [query]\n",
        "    all_similarity = compute_similarity(query, knowledge_base)\n",
        "\n",
        "    results = []\n",
        "    for score, segment in all_similarity:\n",
        "        if score > 0.25:\n",
        "            results.append(segment)\n",
        "\n",
        "    query_passage_pairs = generate_query_passage_pairs(query, results)\n",
        "\n",
        "    if query_passage_pairs:\n",
        "        scores = reranker.compute_score(query_passage_pairs)\n",
        "        sorted_query_passage_pairs = sorted(zip(scores, query_passage_pairs), key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        sorted_results = [(score, pair[1]) for score, pair in sorted_query_passage_pairs]\n",
        "\n",
        "        final_results = []\n",
        "        for score, passage in sorted_results:\n",
        "            # if score > 0:\n",
        "            final_results.append((score, passage))\n",
        "\n",
        "        final_results = final_results[:k]\n",
        "    else:\n",
        "        final_results = [(0.0, 'There is no relevant information for the given query!')]\n",
        "\n",
        "    all_segments = '\\n\\n'.join([result[1] for result in final_results])\n",
        "    class_found = final_syllabus[:-len('.txt')]\n",
        "\n",
        "    return f'Here are class information for {full_course_info[class_found]}:\\n{all_segments}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxaBEOcIw5gh"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def general_news_report(query):\n",
        "    \"\"\"\n",
        "    Fetches news articles based on the user's query.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query for the news topics.\n",
        "        max_articles (int): Maximum number of articles to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted string containing the titles and descriptions of the retrieved articles.\n",
        "    \"\"\"\n",
        "    # Retrieve the News API key from user data or configuration\n",
        "    new_api_key = userdata.get('new_api')  # Ensure this variable is correctly set in your environment\n",
        "    max_articles=5\n",
        "    # News API endpoint\n",
        "    endpoint = 'https://newsapi.org/v2/everything'\n",
        "\n",
        "    # Set query parameters\n",
        "    params = {\n",
        "        'q': query,                # Use the user's query\n",
        "        'language': 'en',          # Language preference\n",
        "        'sortBy': 'publishedAt',   # Order by most recent\n",
        "        'apiKey': new_api_key,\n",
        "    }\n",
        "\n",
        "    # Send request to News API\n",
        "    response = requests.get(endpoint, params=params)\n",
        "\n",
        "    # Check for successful response\n",
        "    if response.status_code == 200:\n",
        "        news_data = response.json()\n",
        "        articles = news_data.get('articles', [])[:max_articles]  # Limit the number of articles\n",
        "        final_res = \"\"\n",
        "\n",
        "        # Parse and append each article's title and description\n",
        "        for article in articles:\n",
        "            title = article.get('title', 'No Title')\n",
        "            description = article.get('description', 'No Description')\n",
        "            final_res += f\"Title: {title}\\n\"\n",
        "            final_res += f\"Description: {description}\\n\\n\"\n",
        "\n",
        "        # Return the formatted string\n",
        "        return final_res.strip()\n",
        "\n",
        "    else:\n",
        "        # Handle errors and return the response status\n",
        "        error_message = f\"Failed to retrieve news: {response.status_code} - {response.reason}\"\n",
        "        print(error_message)\n",
        "        return error_message"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(general_news_report(\"Los Angeles\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZO7sCKyoBaLn",
        "outputId": "0ecfa66f-9c2c-4da0-f150-629efc647a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: [Removed]\n",
            "Description: [Removed]\n",
            "\n",
            "Title: Nelly Knows “Where The Party At” with His 2025 World Tour\n",
            "Description: Celebrating the 25th anniversary of his groundbreaking debut album, Country Grammar, music superstar and three-time Grammy winner Nelly is set to take the party global in 2025. The \"Where The Party At Tour\" promises over 50 electrifying dates across four cont…\n",
            "\n",
            "Title: MISTER CARTOON x XLARGE Capsule Collection\n",
            "Description: The legendary streetwear brand XLARGE has teamed with another street legend, MISTER CARTOON, on a special capsule collection that is available now. XLARGE was a pioneer of street culture in the 1990s, and CARTOON is a hybrid of so much of LA's culture, from s…\n",
            "\n",
            "Title: Could Campus Activity Involvement Keep Students in College?\n",
            "Description: Could Campus Activity Involvement Keep Students in College?\n",
            "\n",
            " \n",
            "\n",
            "\n",
            " \n",
            "Melissa Ezarik\n",
            "\n",
            "Tue, 12/10/2024 - 03:00 AM\n",
            "\n",
            "Student Voice data looks at what the college experience was like for those who stopped out. Their lack of engagement in activities compared to curre…\n",
            "\n",
            "Title: Oppenheimer Issues Positive Forecast for Porch Group (NASDAQ:PRCH) Stock Price\n",
            "Description: Porch Group (NASDAQ:PRCH – Free Report) had its target price lifted by Oppenheimer from $4.00 to $7.00 in a research note released on Monday morning,Benzinga reports. They currently have an outperform rating on the stock. Other analysts have also issued resea…\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def fetch_weather(params) -> str:\n",
        "    \"\"\"\n",
        "    Fetch weather information based on the provided location and date.\n",
        "\n",
        "    Args:\n",
        "        params (str): A string containing the location and date in the format 'Location, Date'.\n",
        "\n",
        "    Returns:\n",
        "        str: A stringified dictionary containing the weather details or an error/warning message.\n",
        "    \"\"\"\n",
        "    # Expecting params as a string, e.g., 'Los Angeles, December 5, 2024'\n",
        "    if not isinstance(params, str):\n",
        "        return \"Error: Invalid input format. Expected a string 'Location, Date'.\"\n",
        "\n",
        "    try:\n",
        "        # Split the input into location and date\n",
        "        location, date_str = map(str.strip, params.split(',', 1))\n",
        "        # Parse the date into a standardized format\n",
        "        query_date = datetime.strptime(date_str, \"%B %d, %Y\").date()\n",
        "    except ValueError:\n",
        "        return \"Error: 'params' format is invalid. Expected format: 'Location, Month Day, Year'.\"\n",
        "\n",
        "    # Validate the query date (ensure it is not beyond the forecast range)\n",
        "    today = datetime.now().date()\n",
        "    max_forecast_date = today + timedelta(days=3)  # wttr.in provides a 3-day forecast\n",
        "    if query_date > max_forecast_date:\n",
        "        return f\"Warning: Weather data for {query_date} is not available. Please provide a date within the next 3 days.\"\n",
        "\n",
        "    # Make the API call to wttr.in\n",
        "    try:\n",
        "        response = requests.get(f\"https://wttr.in/{location}?format=j1\", timeout=10)\n",
        "        response.raise_for_status()\n",
        "        weather_data = response.json()\n",
        "    except requests.RequestException as e:\n",
        "        return f\"Error fetching weather data: {e}\"\n",
        "\n",
        "    # Handle current and forecast weather\n",
        "    try:\n",
        "        if query_date == today:\n",
        "            # Return current weather\n",
        "            current_condition = weather_data.get(\"current_condition\", [{}])[0]\n",
        "            result = {\n",
        "                \"FeelsLikeC\": current_condition.get(\"FeelsLikeC\", \"N/A\"),\n",
        "                \"temp_C\": current_condition.get(\"temp_C\", \"N/A\"),\n",
        "                \"weatherDesc\": current_condition.get(\"weatherDesc\", [{}])[0].get(\"value\", \"N/A\"),\n",
        "                \"humidity\": current_condition.get(\"humidity\", \"N/A\"),\n",
        "            }\n",
        "            return f\"Current weather for {location}: {result}\"\n",
        "        else:\n",
        "            # Return forecast weather\n",
        "            forecast = weather_data.get(\"weather\", [])\n",
        "            for day_forecast in forecast:\n",
        "                forecast_date = datetime.strptime(day_forecast[\"date\"], \"%Y-%m-%d\").date()\n",
        "                if forecast_date == query_date:\n",
        "                    hourly_data = day_forecast.get(\"hourly\", [{}])\n",
        "                    result = {\n",
        "                        \"max_temp\": day_forecast.get(\"maxtempC\", \"N/A\"),\n",
        "                        \"min_temp\": day_forecast.get(\"mintempC\", \"N/A\"),\n",
        "                        \"description\": hourly_data[0].get(\"weatherDesc\", [{}])[0].get(\"value\", \"N/A\"),\n",
        "                    }\n",
        "                    return f\"Forecast weather for {location} on {query_date}: {result}\"\n",
        "            return f\"Weather forecast for {query_date} is not available. Please try another date.\"\n",
        "    except KeyError as e:\n",
        "        return f\"Error processing weather data: Key {e} not found in the response.\""
      ],
      "metadata": {
        "id": "RvxNPCsEtWG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPrfNQhUAVS_"
      },
      "outputs": [],
      "source": [
        "tools_list = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"RAG\",\n",
        "            \"description\": \"Retrieve the relevant section in the given knowledge base when the user asks information about courses or syllabus.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"query\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The original question that the user asks exactly, no need to rephrase.\",\n",
        "                    },\n",
        "                },\n",
        "                \"required\": [\"query\"],\n",
        "                \"additionalProperties\": False,\n",
        "            },\n",
        "        }\n",
        "    },\n",
        "     {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": \"fetch_weather\",\n",
        "                \"description\": \"Fetches the current weather for a specified city with user-defined key selections.\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"params\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": (\n",
        "                                \"A string containing: \"\n",
        "                                \"the name of the city (e.g., 'London') and the Date of searching (e.g., December 5, 2024)\"\n",
        "                            ),\n",
        "                        },\n",
        "                    },\n",
        "                    \"required\": [\"params\"],\n",
        "                    \"additionalProperties\": False,\n",
        "                },\n",
        "            },\n",
        "        },\n",
        "        {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": \"general_news_report\",\n",
        "                \"description\": \"Fetches recent news articles based on the user's query.\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"query\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"The user's search query for the news (e.g., 'artificial intelligence').\",\n",
        "                        },\n",
        "                    },\n",
        "                    \"required\": [\"query\"],\n",
        "                    \"additionalProperties\": False,\n",
        "                },\n",
        "            },\n",
        "        },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpAewj2jlvsq",
        "outputId": "c9701902-46d5-4511-a751-704d13802f1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "toolPrompt = f\"\"\"\n",
        "You have access to the following functions:\n",
        "\n",
        "Use the function '{tools_list[0]['function']['name']}' to '{tools_list[0]['function']['description']}'.\n",
        "The parameters are: {json.dumps(tools_list[0]['function']['parameters']['properties'])}, where {tools_list[0]['function']['parameters']['required']} are required.\n",
        "\n",
        "Use the function '{tools_list[1]['function']['name']}' to '{tools_list[1]['function']['description']}':\n",
        "The parameters are: {json.dumps(tools_list[1]['function']['parameters']['properties'])}, where {tools_list[1]['function']['parameters']['required']} are required.\n",
        "\n",
        "Use the function '{tools_list[2]['function']['name']}' to '{tools_list[2]['function']['description']}':\n",
        "The parameters are: {json.dumps(tools_list[2]['function']['parameters']['properties'])}, where {tools_list[2]['function']['parameters']['required']} are required.\n",
        "\n",
        "If you choose to call a function ONLY reply in the following format with no prefix or suffix:\n",
        "\n",
        "<function=example_function_name>{{\\\"example_name\\\": \\\"example_value\\\"}}</function>\n",
        "\n",
        "Reminder:\n",
        "- Function calls MUST follow the specified format, start with <function= and end with </function>\n",
        "- Required parameters MUST be specified\n",
        "- You can call multiple if user need them\n",
        "- Put the entire function call reply on one line\n",
        "- If there is no function call available, answer the question like normal with your current knowledge and do not tell the user about function calls\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "  \t{\n",
        "        \"role\": \"system\",\n",
        "        \"content\": toolPrompt,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": query,\n",
        "    },\n",
        "\n",
        "]\n",
        "\n",
        "response = together.chat.completions.create(\n",
        "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "    messages=messages,\n",
        "    max_tokens=1024,\n",
        "    temperature=0,\n",
        "    tools=tools_list,\n",
        "    tool_choice=\"auto\",\n",
        ")\n",
        "\n",
        "messages.append(response.choices[0].message)\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gcd_Cgm0AzSi",
        "outputId": "061fae9c-7f8c-4101-f620-59d0a988ccb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You have access to the following functions:\n",
            "\n",
            "Use the function 'RAG' to 'Retrieve the relevant section in the given knowledge base when the user asks information about courses or syllabus.'.\n",
            "The parameters are: {\"query\": {\"type\": \"string\", \"description\": \"The original question that the user asks exactly, no need to rephrase.\"}}, where ['query'] are required.\n",
            "\n",
            "Use the function 'fetch_weather' to 'Fetches the current weather for a specified city with user-defined key selections.':\n",
            "The parameters are: {\"params\": {\"type\": \"string\", \"description\": \"A string containing: the name of the city (e.g., 'London') and the Date of searching (e.g., December 5, 2024)\"}}, where ['params'] are required.\n",
            "\n",
            "Use the function 'general_news_report' to 'Fetches recent news articles based on the user's query.':\n",
            "The parameters are: {\"query\": {\"type\": \"string\", \"description\": \"The user's search query for the news (e.g., 'artificial intelligence').\"}}, where ['query'] are required.\n",
            "\n",
            "If you choose to call a function ONLY reply in the following format with no prefix or suffix:\n",
            "\n",
            "<function=example_function_name>{\"example_name\": \"example_value\"}</function>\n",
            "\n",
            "Reminder:\n",
            "- Function calls MUST follow the specified format, start with <function= and end with </function>\n",
            "- Required parameters MUST be specified\n",
            "- You can call multiple if user need them\n",
            "- Put the entire function call reply on one line\n",
            "- If there is no function call available, answer the question like normal with your current knowledge and do not tell the user about function calls\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(toolPrompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMMpKrEaA22T",
        "outputId": "683c369e-e4b3-4247-e425-ab44f4d0fa3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "role=<MessageRole.ASSISTANT: 'assistant'> content=None tool_calls=[ToolCalls(id='call_h3xojki7cuo8shdjrs326oap', type='function', function=FunctionCall(name='general_news_report', arguments='{\"query\":\"Los Angeles news\"}'), index=0)]\n"
          ]
        }
      ],
      "source": [
        "print(response.choices[0].message)  # -> role=<MessageRole.ASSISTANT: 'assistant'> content=None tool_calls=[ToolCalls(id='call_5wbhh8zyrqa8hf98nw3lqfyu', type='function', function=FunctionCall(name='RAG', arguments='{\"query\":\"csci585 final exam date\"}'), index=0)]\n",
        "# print(response.choices[0].message.tool_calls[0])  # one function at a time -> take index 0 -> id='call_5wbhh8zyrqa8hf98nw3lqfyu' type='function' function=FunctionCall(name='RAG', arguments='{\"query\":\"csci585 final exam date\"}') index=0\n",
        "# print()\n",
        "# print(response.choices[0].message.tool_calls[0].function)  # -> name='RAG' arguments='{\"query\":\"csci585 final exam date\"}'\n",
        "# print()\n",
        "# print(response.choices[0].message.tool_calls[0].function.name)  # Function name -> RAG\n",
        "# print()\n",
        "# print(response.choices[0].message.tool_calls[0].function.arguments)  # Function arguments -> {\"query\":\"csci585 final exam date\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wE5sNZnC7B9Q"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def parse_tool_response(response_message):\n",
        "    \"\"\"\n",
        "    Parses the tool response for function calls and arguments.\n",
        "\n",
        "    Args:\n",
        "        response_message: Response message object with content or tool_calls.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: Parsed response with function name and arguments as a string,\n",
        "                      or None if parsing fails.\n",
        "    \"\"\"\n",
        "    # Check if tool_calls are already provided\n",
        "    if hasattr(response_message, \"tool_calls\") and response_message.tool_calls:\n",
        "        try:\n",
        "            parsed_response = {\n",
        "                \"function\": response_message.tool_calls[0].function.name,\n",
        "                \"arguments\": response_message.tool_calls[0].function.arguments\n",
        "            }\n",
        "            return parsed_response\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing tool_calls arguments: {e}\")\n",
        "            return None\n",
        "\n",
        "    # Regex pattern to extract function calls and arguments\n",
        "    function_regex = r\"<function=([a-zA-Z_]\\w*)>(\\{.*?\\})\\s*(?:</function>|<function(?:/[\\w]*)?>)\"\n",
        "    match = re.search(function_regex, response_message.content)\n",
        "\n",
        "    if match:\n",
        "        function_name, args_string = match.groups()\n",
        "        print(\"Extracted function_name:\", function_name)\n",
        "        print(\"Extracted args_string:\", args_string)\n",
        "\n",
        "        # Simply return the function name and raw arguments as they are valid JSON strings\n",
        "        return {\n",
        "            \"function\": function_name,\n",
        "            \"arguments\": args_string\n",
        "        }\n",
        "\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMqMFlXw8ZzS",
        "outputId": "2418026c-f47c-4ad2-edbf-de68e1a38f4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "role=<MessageRole.ASSISTANT: 'assistant'> content=None tool_calls=[ToolCalls(id='call_h3xojki7cuo8shdjrs326oap', type='function', function=FunctionCall(name='general_news_report', arguments='{\"query\":\"Los Angeles news\"}'), index=0)]\n",
            "The model decide to call a function: \n",
            "{'function': 'general_news_report', 'arguments': '{\"query\":\"Los Angeles news\"}'}\n",
            "{\"query\":\"Los Angeles news\"}\n"
          ]
        }
      ],
      "source": [
        "parsed_response = parse_tool_response(response.choices[0].message)\n",
        "print(response.choices[0].message)\n",
        "if parsed_response is not None:\n",
        "    print('The model decide to call a function: ')\n",
        "    print(parsed_response)\n",
        "else:\n",
        "    print(\"No function call found in the response\")\n",
        "    print(response.choices[0].message.content)\n",
        "print(parsed_response[\"arguments\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NruQN-6VCwV9",
        "outputId": "d5aeb74d-3c5a-4f5c-94d9-9c4fdb349175"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Answer from the LLM:  role=<MessageRole.ASSISTANT: 'assistant'> content='According to the given information, there are several events happening in Los Angeles:\\n\\n1. Pacific Avenue Capital Partners, a private equity firm, has established an office in Paris, France, and has also designated Xavier Lambert as the Head of Europe, with operations set to expand into the EU, although the firm is headquartered in Los Angeles.\\n\\n2. There is no direct information about an \"event\" happening in Los Angeles, but rather Pacific Avenue Capital Partners having its headquarters in Los Angeles.' tool_calls=[]\n"
          ]
        }
      ],
      "source": [
        "if parsed_response:\n",
        "    available_functions = {\n",
        "        \"RAG\": RAG,\n",
        "        \"fetch_weather\": fetch_weather,\n",
        "        \"general_news_report\": general_news_report,\n",
        "    }\n",
        "\n",
        "    if parsed_response[\"function\"] not in available_functions:\n",
        "        available_function_names = \"\\n\".join(available_functions.keys())\n",
        "        raise NotImplementedError(\n",
        "            f\"Function {parsed_response['function']} is not implemented. \"\n",
        "            f\"Our available functions are:\\n\\n{available_function_names}\"\n",
        "        )\n",
        "    try:\n",
        "        arguments = json.loads(parsed_response[\"arguments\"])\n",
        "    except json.JSONDecodeError as e:\n",
        "        raise ValueError(f\"Failed to parse arguments: {e}\")\n",
        "\n",
        "    function_to_call = available_functions[parsed_response[\"function\"]]\n",
        "    # print(function_to_call)\n",
        "    # print(parsed_response[\"arguments\"])\n",
        "    # print(function_to_call(**arguments))\n",
        "    print()\n",
        "\n",
        "    result = function_to_call(**arguments)\n",
        "    # print(\"function call result:\", result)\n",
        "    print()\n",
        "\n",
        "    answer_prompt = \"You are a very helpful assistant. Please answer user's question according to given information. Trust the given information, it is completely align with the user's question.\"\n",
        "\n",
        "    new_messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": answer_prompt,\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"\n",
        "                        ## Question:\n",
        "                        {query}\n",
        "\n",
        "                        ## Information:\n",
        "                        {result}\n",
        "                        \"\"\"\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    res = together.chat.completions.create(\n",
        "        model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "        messages=new_messages,\n",
        "        max_tokens=1000,\n",
        "        temperature=0.9,\n",
        "    )\n",
        "    print(\"Answer from the LLM: \", res.choices[0].message)\n",
        "else:\n",
        "    print(\"No function call found in the response\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxvpSt4MC-W8",
        "outputId": "07ef57c7-40d8-4aaf-b039-9be9cd2632cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a very helpful assistant. Please answer user's question according to given information. Trust the given information, it is completely align with the user's question.\n",
            "\n",
            "                        ## Question:\n",
            "                        what is happening in Los Angeles?\n",
            "\n",
            "                        ## Information:\n",
            "                        Title: Oppenheimer Issues Positive Forecast for Porch Group (NASDAQ:PRCH) Stock Price\n",
            "Description: Porch Group (NASDAQ:PRCH – Free Report) had its target price lifted by Oppenheimer from $4.00 to $7.00 in a research note released on Monday morning,Benzinga reports. They currently have an outperform rating on the stock. Other analysts have also issued resea…\n",
            "\n",
            "Title: Pacific Avenue Capital Partners Establishes Paris France Office and Xavier Lambert Joins as Head of Europe to Lead Firm’s EU Expansion\n",
            "Description: LOS ANGELES & PARIS — Pacific Avenue Capital Partners, LLC (“Pacific Avenue”), a Los Angeles-headquartered private equity firm focused on corporate divestitures, carve-outs, and other complex situations in the middle market, announced today the opening of a E…\n",
            "\n",
            "Title: CBB Bancorp (OTCMKTS:CBBI) versus Svenska Handelsbanken AB (publ) (OTCMKTS:SVNLY) Critical Contrast\n",
            "Description: CBB Bancorp (OTCMKTS:CBBI – Get Free Report) and Svenska Handelsbanken AB (publ) (OTCMKTS:SVNLY – Get Free Report) are both finance companies, but which is the better stock? We will contrast the two businesses based on the strength of their earnings, profitab…\n",
            "\n",
            "Title: Monsters’ Cooper Koch is ‘so grateful’ Kim Kardashian organised Menendez brothers’ prison visit\n",
            "Description: Netflix’s Monsters star Cooper Koch has revealed he’s “grateful” that Kim Kardashian organised for him to visit the real-life Menendez brothers in prison.  Following the release of the controversial but popular dramatisation of the Menendez brothers’ crime an…\n",
            "\n",
            "Title: Cedar Fair (NYSE:FUN) Coverage Initiated by Analysts at Jefferies Financial Group\n",
            "Description: Jefferies Financial Group began coverage on shares of Cedar Fair (NYSE:FUN – Free Report) in a research report released on Monday, Marketbeat.com reports. The firm issued a buy rating and a $59.00 target price on the stock. Several other brokerages have also …\n",
            "                        \n"
          ]
        }
      ],
      "source": [
        "print(answer_prompt)\n",
        "print(new_messages[1]['content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRKM61AEEMgV",
        "outputId": "ecd5fd2c-5796-42b4-cbc0-d7576e09d410"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "According to the given information, there are several events happening in Los Angeles:\n",
            "\n",
            "1. Pacific Avenue Capital Partners, a private equity firm, has established an office in Paris, France, and has also designated Xavier Lambert as the Head of Europe, with operations set to expand into the EU, although the firm is headquartered in Los Angeles.\n",
            "\n",
            "2. There is no direct information about an \"event\" happening in Los Angeles, but rather Pacific Avenue Capital Partners having its headquarters in Los Angeles.\n"
          ]
        }
      ],
      "source": [
        "print(res.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    previous_RAG_info = None  # Placeholder for RAG information from prior turns\n",
        "    conversation_count = 0\n",
        "    chat_history = []  # To store the latest 10 conversations\n",
        "\n",
        "    while True:\n",
        "        # Get user query\n",
        "        user_query = input(\"User: \")\n",
        "\n",
        "        if user_query.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Exiting conversation.\")\n",
        "            break\n",
        "        use_previous_RAG = False\n",
        "        # Decide if previous RAG info should be used (dummy condition for now)\n",
        "        if conversation_count > 0 and previous_RAG_info:\n",
        "            use_previous_RAG = False  # Replace with actual condition\n",
        "        else:\n",
        "            use_previous_RAG = False\n",
        "\n",
        "        if use_previous_RAG:\n",
        "            user_query = f\"{user_query}\\n\\nAdditional context:\\n{previous_RAG_info}\"\n",
        "\n",
        "        # Add the new user query to the chat history\n",
        "        chat_history.append({\"role\": \"user\", \"content\": user_query})\n",
        "\n",
        "        # Ensure the chat history is limited to the latest 10 exchanges\n",
        "        if len(chat_history) > 20:  # Each \"exchange\" is a user+assistant pair\n",
        "            chat_history = chat_history[-20:]\n",
        "\n",
        "        # Construct messages with chat history\n",
        "        messages = [{\"role\": \"system\", \"content\": toolPrompt}] + chat_history\n",
        "\n",
        "        # Call the LLM\n",
        "        response = together.chat.completions.create(\n",
        "            model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "            messages=messages,\n",
        "            max_tokens=1024,\n",
        "            temperature=0,\n",
        "            tools=tools_list,\n",
        "            tool_choice=\"auto\",\n",
        "        )\n",
        "\n",
        "        # Parse the response\n",
        "        response_message = response.choices[0].message\n",
        "        parsed_response = parse_tool_response(response_message)\n",
        "\n",
        "        # Add assistant's response to chat history\n",
        "        if parsed_response:\n",
        "            available_functions = {\n",
        "                \"RAG\": RAG,\n",
        "                \"fetch_weather\": fetch_weather,\n",
        "                \"general_news_report\": general_news_report,\n",
        "            }\n",
        "            function_name = parsed_response[\"function\"]\n",
        "\n",
        "            arguments = json.loads(parsed_response[\"arguments\"])\n",
        "            if function_name in available_functions:\n",
        "                result = available_functions[function_name](**arguments) #应该是车越的意思\n",
        "                if function_name == \"RAG\":\n",
        "                    previous_RAG_info = result  # Store RAG info for future turns\n",
        "                    result += \"Location: Los Angeles\"\n",
        "            else:\n",
        "                print(f\"Unknown function: {function_name}\")\n",
        "\n",
        "            # Prepare the answer prompt for the second LLM call\n",
        "            answer_prompt = (\n",
        "                \"You are a very helpful assistant. Please answer the user's question using the information provided below. \"\n",
        "                \"Trust the given information, as it is aligned with the user's query.\\n\\n\"\n",
        "                f\"## Question:\\n{user_query}\\n\\n\"\n",
        "                f\"## Information:\\n{result}\"\n",
        "            )\n",
        "\n",
        "            # Create new LLM messages\n",
        "            new_messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
        "            new_messages += chat_history\n",
        "            new_messages.append({\"role\": \"user\", \"content\": answer_prompt})\n",
        "\n",
        "            # Call the LLM again with the tool's result\n",
        "            final_response = together.chat.completions.create(\n",
        "                model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "                messages=new_messages,\n",
        "                max_tokens=1000,\n",
        "                temperature=0.9,\n",
        "            )\n",
        "            assistant_response = final_response.choices[0].message.content\n",
        "            print(\"Agent:\", assistant_response)\n",
        "            chat_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "        else:\n",
        "            messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
        "            messages += chat_history\n",
        "            messages.append({\"role\": \"user\", \"content\": user_query})\n",
        "\n",
        "            # Call the LLM to generate a response\n",
        "            response = together.chat.completions.create(\n",
        "                model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "                messages=messages,\n",
        "                max_tokens=1000,\n",
        "                temperature=0.9,\n",
        "            )\n",
        "            assistant_response = response.choices[0].message.content\n",
        "            print(\"Agent:\", assistant_response)\n",
        "            chat_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "\n",
        "        print()\n",
        "        print(\"**********split**********\")\n",
        "        print()\n",
        "\n",
        "        # Increment conversation count\n",
        "        conversation_count += 1\n"
      ],
      "metadata": {
        "id": "1NliEWQeIQp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yt4Ya1rIbgq",
        "outputId": "62576a43-8e0a-4900-9923-e1f7c56a3f8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: quit\n",
            "Exiting conversation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main_MultiFunction():\n",
        "    previous_RAG_info = None  # Placeholder for RAG information from prior turns\n",
        "    conversation_count = 0\n",
        "    chat_history = []  # To store the latest 10 conversations\n",
        "\n",
        "    while True:\n",
        "        # Get user query\n",
        "        user_query = input(\"User: \")\n",
        "\n",
        "        if user_query.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Exiting conversation.\")\n",
        "            break\n",
        "        use_previous_RAG = False\n",
        "        # Decide if previous RAG info should be used (dummy condition for now)\n",
        "        if conversation_count > 0 and previous_RAG_info:\n",
        "            use_previous_RAG = False  # Replace with actual condition\n",
        "        else:\n",
        "            use_previous_RAG = False\n",
        "\n",
        "        if use_previous_RAG:\n",
        "            user_query = f\"{user_query}\\n\\nAdditional context:\\n{previous_RAG_info}\"\n",
        "\n",
        "        # Add the new user query to the chat history\n",
        "        chat_history.append({\"role\": \"user\", \"content\": user_query})\n",
        "\n",
        "        # Ensure the chat history is limited to the latest 10 exchanges\n",
        "        if len(chat_history) > 20:  # Each \"exchange\" is a user+assistant pair\n",
        "            chat_history = chat_history[-20:]\n",
        "\n",
        "        # Construct messages with chat history\n",
        "        messages = [{\"role\": \"system\", \"content\": toolPrompt}] + chat_history\n",
        "\n",
        "        # Call the LLM\n",
        "        response = together.chat.completions.create(\n",
        "            model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "            messages=messages,\n",
        "            max_tokens=1024,\n",
        "            temperature=0,\n",
        "            tools=tools_list,\n",
        "            tool_choice=\"auto\",\n",
        "        )\n",
        "\n",
        "        # Parse the response\n",
        "        response_message = response.choices[0].message\n",
        "        parsed_response = parse_tool_response(response_message)\n",
        "        print(\"parsed_response\",parsed_response)\n",
        "        # Add assistant's response to chat history\n",
        "        if parsed_response:\n",
        "            available_functions = {\n",
        "                \"RAG\": RAG,\n",
        "                \"fetch_weather\": fetch_weather,\n",
        "                \"general_news_report\": general_news_report,\n",
        "            }\n",
        "            tools_to_process = []  # A list to handle sequential tool calls\n",
        "            arguments = None\n",
        "            result = None\n",
        "            while_out = 0\n",
        "            # Handle the first function and queue the next if necessary\n",
        "            while parsed_response:\n",
        "                while_out += 1\n",
        "                if while_out > 1:\n",
        "                    break\n",
        "                function_name = parsed_response[\"function\"]\n",
        "                print(function_name,\"function name\")\n",
        "                arguments = json.loads(parsed_response[\"arguments\"])\n",
        "                print(arguments,\"arguments\")\n",
        "                if function_name in available_functions:\n",
        "                    result = available_functions[function_name](**arguments)\n",
        "                    if function_name == \"RAG\":\n",
        "                        previous_RAG_info = result  # Store RAG info for future turns\n",
        "                        result += \"END RAG RESULT\\tLocation: Los Angeles \\t\"\n",
        "                    tools_to_process.append({\"function\": function_name, \"result\": result})\n",
        "\n",
        "                    # If more tools are needed, ask the agent to infer the next step\n",
        "                    new_tool_prompt = (\n",
        "\n",
        "                        \"Based on the result of the tool call, determine if additional functions are needed. Do not Repeat Function Call! \"\n",
        "                        \"If needed, provide the function name and arguments and call the function again thank you!\"\n",
        "                    )\n",
        "                    next_tool_messages = [{\"role\": \"system\", \"content\": new_tool_prompt}]\n",
        "                    next_tool_messages.append({\"role\": \"user\", \"content\": f\"You have used {tools_to_process[-1]}, and result for it is {result}\" + f\" Do not Repeat Function Call! This is user query again : {user_query}. This is Tool use list: {toolPrompt}\"})\n",
        "\n",
        "                    next_tool_response = together.chat.completions.create(\n",
        "                        model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
        "                        messages=next_tool_messages,\n",
        "                        max_tokens=500,\n",
        "                        tools=tools_list,\n",
        "                        temperature=0,\n",
        "                    )\n",
        "                    # print(next_tool_messages,\"prompt\")\n",
        "                    parsed_response = parse_tool_response(next_tool_response.choices[0].message)\n",
        "\n",
        "                    # print(parsed_response,\"inside while loop\")\n",
        "                else:\n",
        "                    print(f\"Unknown function: {function_name}\")\n",
        "                    break\n",
        "\n",
        "            # Use the final tool result to generate the user-facing response\n",
        "            final_result_summary = \"\\n\".join(\n",
        "                [f\"Tool: {tool['function']}\\nResult: {tool['result']}\" for tool in tools_to_process]\n",
        "            )\n",
        "            final_response_prompt = (\n",
        "                \"Based on the following tool results, provide a helpful answer to the user's query:\\n\\n\"\n",
        "                f\"{final_result_summary}\\n\\nUser query:\\n{user_query}\"\n",
        "            )\n",
        "\n",
        "            # Create final response\n",
        "            final_messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant. Below is a series of history conversation\"}]\n",
        "            final_messages += chat_history\n",
        "            final_messages.append({\"role\": \"user\", \"content\": final_response_prompt})\n",
        "\n",
        "            final_response = together.chat.completions.create(\n",
        "                model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "                messages=final_messages,\n",
        "                max_tokens=1000,\n",
        "                temperature=0.9,\n",
        "            )\n",
        "            assistant_response = final_response.choices[0].message.content\n",
        "            print(\"Agent:\", assistant_response)\n",
        "            chat_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "        else:\n",
        "            # Direct LLM response\n",
        "            messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
        "            messages += chat_history\n",
        "            messages.append({\"role\": \"user\", \"content\": user_query})\n",
        "\n",
        "            # Call the LLM to generate a response\n",
        "            response = together.chat.completions.create(\n",
        "                model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "                messages=messages,\n",
        "                max_tokens=1000,\n",
        "                temperature=0.9,\n",
        "            )\n",
        "            assistant_response = response.choices[0].message.content\n",
        "            print(\"Agent:\", assistant_response)\n",
        "            chat_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "\n",
        "        print()\n",
        "        print(\"**********split**********\")\n",
        "        print()\n",
        "\n",
        "        # Increment conversation count\n",
        "        conversation_count += 1\n"
      ],
      "metadata": {
        "id": "RJaB1kaSfkfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_MultiFunction()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COu9uvHQfxyM",
        "outputId": "c662ca60-16b2-4d8a-ab1a-3caa17e65c9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Hi I wonder for the CSCI 544 when is the final exam date? Also, if the day going to be a sunny day?\n",
            "parsed_response None\n",
            "Agent: I'd be happy to help you with that, but I'm a large language model, I don't have real-time access to specific information about your university or course schedule.\n",
            "\n",
            "However, I can suggest a few possible ways to find out the final exam date for CSCI 544:\n",
            "\n",
            "1. Check the university's course catalog or website: You can typically find course schedules and exam dates in the course catalog or on the university's website.\n",
            "2. Contact the department: You can reach out to the Computer Science department at your university and ask for the final exam date for CSCI 544.\n",
            "3. Check with your instructor: If you have your instructor's contact information, you can reach out to them directly and ask for the final exam date.\n",
            "\n",
            "As for the weather, I can provide you with general information about sunny days, but I won't be able to predict the weather for a specific location or date. You can check a weather website or app like AccuWeather, Weather.com, or the Weather Channel for more accurate and up-to-date weather forecasts.\n",
            "\n",
            "If you have any more information or if there's anything else I can help you with, feel free to ask!\n",
            "\n",
            "**********split**********\n",
            "\n",
            "User: Hi I wonder for the CSCI 544 when is the final exam date? Also, if the day going to be a sunny day?\n",
            "parsed_response {'function': 'RAG', 'arguments': '{\"query\":\"CSCI 544 final exam date and weather on the day of the final exam\"}'}\n",
            "RAG function name\n",
            "{'query': 'CSCI 544 final exam date and weather on the day of the final exam'} arguments\n",
            "CSCI544.txt\n",
            "[{'role': 'system', 'content': 'Based on the result of the tool call, determine if additional functions are needed. Do not Repeat Function Call! If needed, provide the function name and arguments and call the function again thank you!'}, {'role': 'user', 'content': 'You have used {\\'function\\': \\'RAG\\', \\'result\\': \\'Here are class information for CSCI544 Applied Natural Language Processing (NLP):\\\\nExams and Important Dates:\\\\nMidterm: October 15, 2024\\\\nFinal Exam: December 5, 2024\\\\nFinal Project Report Due: December 17, 2024\\\\n\\\\nGrading:\\\\nQuizzes: 10%\\\\nHomework: 20%\\\\nPaper Presentations: 5%\\\\nCourse Project: 40%\\\\nMidterm Exam: 10%\\\\nFinal Exam: 15%\\\\n\\\\nGraders: Shahzaib Saqib Warraich, Neeharika Gupta, Xingjian Dong, Smit ShahEND RAG RESULT\\\\tLocation: Los Angeles \\\\t\\'}, and result for it is Here are class information for CSCI544 Applied Natural Language Processing (NLP):\\nExams and Important Dates:\\nMidterm: October 15, 2024\\nFinal Exam: December 5, 2024\\nFinal Project Report Due: December 17, 2024\\n\\nGrading:\\nQuizzes: 10%\\nHomework: 20%\\nPaper Presentations: 5%\\nCourse Project: 40%\\nMidterm Exam: 10%\\nFinal Exam: 15%\\n\\nGraders: Shahzaib Saqib Warraich, Neeharika Gupta, Xingjian Dong, Smit ShahEND RAG RESULT\\tLocation: Los Angeles \\t Do not Repeat Function Call! This is user query again : Hi I wonder for the CSCI 544 when is the final exam date? Also, if the day going to be a sunny day?. This is Tool use list: \\nYou have access to the following functions:\\n\\nUse the function \\'RAG\\' to \\'Retrieve the relevant section in the given knowledge base when the user asks information about courses or syllabus.\\'.\\nThe parameters are: {\"query\": {\"type\": \"string\", \"description\": \"The original question that the user asks exactly, no need to rephrase.\"}}, where [\\'query\\'] are required.\\n\\nUse the function \\'fetch_weather\\' to \\'Fetches the current weather for a specified city with user-defined key selections.\\':\\nThe parameters are: {\"params\": {\"type\": \"string\", \"description\": \"A string containing: the name of the city (e.g., \\'London\\') and the Date of searching (e.g., December 5, 2024)\"}}, where [\\'params\\'] are required.\\n\\nUse the function \\'general_news_report\\' to \\'Fetches recent news articles based on the user\\'s query.\\':\\nThe parameters are: {\"query\": {\"type\": \"string\", \"description\": \"The user\\'s search query for the news (e.g., \\'artificial intelligence\\').\"}}, where [\\'query\\'] are required.\\n\\nIf you choose to call a function ONLY reply in the following format with no prefix or suffix:\\n\\n<function=example_function_name>{\"example_name\": \"example_value\"}</function>\\n\\nReminder:\\n- Function calls MUST follow the specified format, start with <function= and end with </function>\\n- Required parameters MUST be specified\\n- You can call multiple if user need them\\n- Put the entire function call reply on one line\\n- If there is no function call available, answer the question like normal with your current knowledge and do not tell the user about function calls\\n\\n'}] prompt\n",
            "{'function': 'fetch_weather', 'arguments': '{\"params\":\"Los Angeles, December 5, 2024\"}'} inside while loop\n",
            "Agent: Based on the RAG tool results, I can provide you with the following information:\n",
            "\n",
            "**Final Exam Date for CSCI 544:** The final exam date for CSCI 544 Applied Natural Language Processing (NLP) is December 5, 2024.\n",
            "\n",
            "As for the weather, I don't have any information about the weather forecast for a specific location on a specific date. However, I can suggest checking a weather website or app for more accurate and up-to-date weather forecasts.\n",
            "\n",
            "If you'd like to know the weather forecast for Los Angeles on December 5, 2024, you can check a website like AccuWeather or Weather.com for more information.\n",
            "\n",
            "**********split**********\n",
            "\n",
            "User: quit\n",
            "Exiting conversation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main_MultiFunction():\n",
        "    previous_RAG_info = None  # Placeholder for RAG information from prior turns\n",
        "    conversation_count = 0\n",
        "    chat_history = []  # To store the latest 10 conversations\n",
        "    executed_calls = set()  # Track executed function calls\n",
        "\n",
        "    while True:\n",
        "        # Get user query\n",
        "        user_query = input(\"User: \")\n",
        "\n",
        "        if user_query.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"Exiting conversation.\")\n",
        "            break\n",
        "\n",
        "        use_previous_RAG = False\n",
        "        # Decide if previous RAG info should be used (dummy condition for now)\n",
        "        if conversation_count > 0 and previous_RAG_info:\n",
        "            use_previous_RAG = False  # Replace with actual condition\n",
        "        else:\n",
        "            use_previous_RAG = False\n",
        "\n",
        "        if use_previous_RAG:\n",
        "            user_query = f\"{user_query}\\n\\nAdditional context:\\n{previous_RAG_info}\"\n",
        "\n",
        "        # Add the new user query to the chat history\n",
        "        chat_history.append({\"role\": \"user\", \"content\": user_query})\n",
        "\n",
        "        # Ensure the chat history is limited to the latest 10 exchanges\n",
        "        if len(chat_history) > 20:  # Each \"exchange\" is a user+assistant pair\n",
        "            chat_history = chat_history[-20:]\n",
        "\n",
        "        # Construct messages with chat history\n",
        "        messages = [{\"role\": \"system\", \"content\": toolPrompt}] + chat_history\n",
        "\n",
        "        # Call the LLM\n",
        "        response = together.chat.completions.create(\n",
        "            model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "            messages=messages,\n",
        "            max_tokens=1024,\n",
        "            temperature=0,\n",
        "            tools=tools_list,\n",
        "            tool_choice=\"auto\",\n",
        "        )\n",
        "\n",
        "        # Parse the response\n",
        "        response_message = response.choices[0].message\n",
        "        parsed_response = parse_tool_response(response_message)\n",
        "        print(\"parsed_response\", parsed_response)\n",
        "\n",
        "        if parsed_response:\n",
        "            available_functions = {\n",
        "                \"RAG\": RAG,\n",
        "                \"fetch_weather\": fetch_weather,\n",
        "                \"general_news_report\": general_news_report,\n",
        "            }\n",
        "            tools_to_process = []  # A list to handle sequential tool calls\n",
        "            result = None\n",
        "\n",
        "            while_out = 0\n",
        "            while parsed_response:\n",
        "                while_out += 1\n",
        "                if while_out > 3:  # Prevent infinite loops\n",
        "                    print(\"Breaking loop due to excessive iterations.\")\n",
        "                    break\n",
        "\n",
        "                function_name = parsed_response[\"function\"]\n",
        "                arguments = json.dumps(parsed_response[\"arguments\"])  # Serialize arguments to track as a key\n",
        "\n",
        "                if (function_name, arguments) in executed_calls:\n",
        "                    # print(f\"Skipping repeated function call: {function_name} with arguments: {arguments}\")\n",
        "                    break  # Prevent repeated calls\n",
        "\n",
        "                executed_calls.add((function_name, arguments))  # Add to executed calls\n",
        "                # print(function_name, \"function name\")\n",
        "                # print(arguments, \"arguments\")\n",
        "\n",
        "                if function_name in available_functions:\n",
        "                    result = available_functions[function_name](**json.loads(arguments))\n",
        "                    if function_name == \"RAG\":\n",
        "                        previous_RAG_info = result  # Store RAG info for future turns\n",
        "                        result += \"END RAG RESULT\\tLocation: Los Angeles \\t\"\n",
        "                    tools_to_process.append({\"function\": function_name, \"result\": result})\n",
        "\n",
        "                    # Determine next tool call\n",
        "                    new_tool_prompt = (\n",
        "                        \"Based on the result of the tool call, determine if additional functions are needed. Do not Repeat Function Call! \"\n",
        "                        \"If needed, provide the function name and arguments and call the function again thank you!\"\n",
        "                    )\n",
        "                    next_tool_messages = [{\"role\": \"system\", \"content\": new_tool_prompt}]\n",
        "                    next_tool_messages += chat_history\n",
        "                    next_tool_messages.append({\"role\": \"user\", \"content\": f\"tool_result: {result}\"})\n",
        "\n",
        "                    next_tool_response = together.chat.completions.create(\n",
        "                        model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "                        messages=next_tool_messages,\n",
        "                        max_tokens=500,\n",
        "                        tools=tools_list,\n",
        "                        temperature=0,\n",
        "                    )\n",
        "                    # print(next_tool_messages, \"prompt\")\n",
        "                    next_tool_parsed = parse_tool_response(next_tool_response.choices[0].message)\n",
        "                    parsed_response = next_tool_parsed\n",
        "                    # print(parsed_response, \"inside while loop\")\n",
        "                else:\n",
        "                    print(f\"Unknown function: {function_name}\")\n",
        "                    break\n",
        "\n",
        "            # Generate user-facing response\n",
        "            final_result_summary = \"\\n\".join(\n",
        "                [f\"Tool: {tool['function']}\\nResult: {tool['result']}\" for tool in tools_to_process]\n",
        "            )\n",
        "            final_response_prompt = (\n",
        "                \"Based on the following tool results, provide a helpful answer to the user's query:\\n\\n\"\n",
        "                f\"{final_result_summary}\\n\\nUser query:\\n{user_query}\"\n",
        "            )\n",
        "\n",
        "            final_messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant. Below is a series of history conversation\"}]\n",
        "            final_messages += chat_history\n",
        "            final_messages.append({\"role\": \"user\", \"content\": final_response_prompt})\n",
        "\n",
        "            final_response = together.chat.completions.create(\n",
        "                model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "                messages=final_messages,\n",
        "                max_tokens=1000,\n",
        "                temperature=0.9,\n",
        "            )\n",
        "            assistant_response = final_response.choices[0].message.content\n",
        "            print(\"Agent:\", assistant_response)\n",
        "            chat_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "        else:\n",
        "            # Direct response\n",
        "            messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
        "            messages += chat_history\n",
        "            messages.append({\"role\": \"user\", \"content\": user_query})\n",
        "\n",
        "            response = together.chat.completions.create(\n",
        "                model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "                messages=messages,\n",
        "                max_tokens=1000,\n",
        "                temperature=0.9,\n",
        "            )\n",
        "            assistant_response = response.choices[0].message.content\n",
        "            print(\"Agent:\", assistant_response)\n",
        "            chat_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "\n",
        "        print(\"\\n**********split**********\\n\")\n",
        "        conversation_count += 1\n"
      ],
      "metadata": {
        "id": "b2C9zQzbfyqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(fetch_weather('Los Angeles, December 5, 2024'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUrmVwsLsCpl",
        "outputId": "ef06bedb-54e8-412b-a336-53530d287d9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weather forecast for 2024-12-05 is not available. Please try another date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(fetch_weather('Los Angeles, December 11, 2024'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqMtp0cciziv",
        "outputId": "f0df36e2-9459-4131-91c5-e255f6c06aff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forecast weather for Los Angeles on 2024-12-11: {'max_temp': '22', 'min_temp': '10', 'description': 'Clear '}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MpVcpZ6wBJpM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}