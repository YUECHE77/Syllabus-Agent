{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "F_eE_DNsARvt"
   },
   "outputs": [],
   "source": [
    "!pip install -q together\n",
    "!pip install -q FlagEmbedding\n",
    "!pip install -q peft\n",
    "!pip install -q faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14UzEZT8-8L2",
    "outputId": "0f41e71c-95eb-4aac-d85a-d4a2041dfb49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aMd5gnmYlWmp"
   },
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagModel\n",
    "from FlagEmbedding import FlagReranker\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import faiss\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "d1SGcTmOYcCi"
   },
   "outputs": [],
   "source": [
    "# encode_queries()和encode()的区别：\n",
    "# for s2p (short query to long passage) retrieval task, suggest to use encode_queries() which will automatically add the instruction to each query\n",
    "# corpus in retrieval task can still use encode() or encode_corpus(), since they don't need instruction\n",
    "\n",
    "model = FlagModel('BAAI/bge-base-en-v1.5', query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\", use_fp16=True)\n",
    "# print(model.device)\n",
    "\n",
    "reranker = FlagReranker('BAAI/bge-reranker-base', use_fp16=True, device='cuda')\n",
    "# print(reranker.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mgg1y2v2VtZ6"
   },
   "outputs": [],
   "source": [
    "# query = \"Tell me the exam information for the database class.\"\n",
    "query = \"what is happening in Los Angeles?\"\n",
    "\n",
    "courses_numbers = ['544', '566', '585', '596', '599', '626', '677', '699']\n",
    "course_names = ['Applied Natural Language Processing (NLP)', 'Deep Learning and its Applications (DL)', 'Database Systems (database)', 'Scientific Computing and Visualization',\n",
    "         'Distributed Systems', 'Text as Data', 'Advanced Computer Vision (CV)', 'Robotic Perception (Robotics)']\n",
    "\n",
    "full_course_info = dict(zip(['CSCI' + num for num in courses_numbers], ['CSCI' + num + ' ' + name for num, name in zip(courses_numbers, course_names)]))\n",
    "\n",
    "name_to_num = dict(zip(course_names, courses_numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hcfesIpb24G1",
    "outputId": "463e3683-6911-42a7-bb45-e959219d2238"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CSCI544': 'CSCI544 Applied Natural Language Processing (NLP)',\n",
       " 'CSCI566': 'CSCI566 Deep Learning and its Applications (DL)',\n",
       " 'CSCI585': 'CSCI585 Database Systems (database)',\n",
       " 'CSCI596': 'CSCI596 Scientific Computing and Visualization',\n",
       " 'CSCI599': 'CSCI599 Distributed Systems',\n",
       " 'CSCI626': 'CSCI626 Text as Data',\n",
       " 'CSCI677': 'CSCI677 Advanced Computer Vision (CV)',\n",
       " 'CSCI699': 'CSCI699 Robotic Perception (Robotics)'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_course_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RzdFZ3LiZIji",
    "outputId": "9747b524-7ad9-4d0e-998a-ea923007b0f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "def build_vector_database(course_names):\n",
    "    course_embeddings = model.encode(course_names)\n",
    "    np_course_embeddings = np.array(course_embeddings).astype('float32')\n",
    "\n",
    "    # with normalization -> bacome cosine similarity search\n",
    "    faiss.normalize_L2(np_course_embeddings)\n",
    "\n",
    "    # IndexFlatIP: dot-product search, IndexFlatL2: L2 search\n",
    "    index_innerproduct = faiss.IndexFlatIP(len(course_embeddings[0]))\n",
    "    res = faiss.StandardGpuResources()  # Create GPU resources\n",
    "    gpu_index = faiss.index_cpu_to_gpu(res, 0, index_innerproduct) # Move index to GPU\n",
    "\n",
    "    gpu_index.add(np_course_embeddings)\n",
    "\n",
    "    return gpu_index\n",
    "\n",
    "database = build_vector_database(course_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PcvkLGk4ZbFi"
   },
   "outputs": [],
   "source": [
    "def search_(query, database, embed_dim, topk=1):\n",
    "    query_embed = model.encode_queries(query)\n",
    "    query_embed = np.array(query_embed).astype('float32')\n",
    "\n",
    "    _, idx = database.search(query_embed.reshape((1, embed_dim)), topk) # (768,) -> (1, 768)\n",
    "    idx = idx.reshape(-1)\n",
    "\n",
    "    ret = [course_names[i] for i in idx]\n",
    "\n",
    "    return ret\n",
    "\n",
    "def find_syllabus(query, courses_numbers, course_names, name_to_num, model, database):\n",
    "    def find_course():\n",
    "        for num in courses_numbers:\n",
    "            if num in query:\n",
    "                return num\n",
    "\n",
    "        result = search_(query, database, embed_dim=database.d, topk=1)\n",
    "        return result[0]\n",
    "\n",
    "    result = find_course()\n",
    "\n",
    "    if result in courses_numbers:\n",
    "        return 'CSCI' + result + '.txt'\n",
    "    else:\n",
    "        return 'CSCI' + name_to_num[result] + '.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpc548UOjMQR"
   },
   "outputs": [],
   "source": [
    "print(find_syllabus(query, courses_numbers, course_names, name_to_num, model, database))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEdJxWWpaPm1"
   },
   "outputs": [],
   "source": [
    "# 读取txt文件并按\"\\n\\n\"切割 -> 按照需求自行修改！！！\n",
    "def read_and_split_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    segments = content.split('##')\n",
    "    # print(segments)\n",
    "\n",
    "    return [seg.strip() for seg in segments]  # 变成了一个list\n",
    "\n",
    "# 计算query和每个文档分段的相似度，并返回排序后的结果\n",
    "def compute_similarity(query, segments):\n",
    "    # encode_queries是专门用于encode query的\n",
    "    query_embedding = model.encode_queries(query)  # [512, ]\n",
    "    segment_embeddings = model.encode(segments)  # [num_seg, 512]\n",
    "\n",
    "    # 官方提供的。只计算点积，不计算cos-simi\n",
    "    similarities = [query_embedding @ segment_embedding.T for segment_embedding in segment_embeddings]\n",
    "\n",
    "    # 将分数、query和文档分段组合并排序 -> 降序\n",
    "    results = sorted(zip(similarities, segments), key=lambda x: x[0], reverse=True)\n",
    "    # print(results)\n",
    "\n",
    "    return results\n",
    "\n",
    "# 定义生成查询和文档组合的函数\n",
    "def generate_query_passage_pairs(query, passages):\n",
    "    return [[query[0], passage] for passage in passages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QOAOWd0YV4CE"
   },
   "outputs": [],
   "source": [
    "def RAG(query):\n",
    "\n",
    "    if query is None:\n",
    "        raise ValueError('Your query cannot be empty!')\n",
    "\n",
    "    k = 3\n",
    "\n",
    "    final_syllabus = find_syllabus(query, courses_numbers, course_names, name_to_num, model, database)\n",
    "    print(final_syllabus)\n",
    "\n",
    "    # file_path = '/content' + os.sep + final_syllabus\n",
    "    file_path = '/content/drive/MyDrive/CSCI544/Project/syllabus/' + final_syllabus\n",
    "    knowledge_base = read_and_split_file(file_path)\n",
    "\n",
    "    query = [query]\n",
    "    all_similarity = compute_similarity(query, knowledge_base)\n",
    "\n",
    "    results = []\n",
    "    for score, segment in all_similarity:\n",
    "        if score > 0.25:\n",
    "            results.append(segment)\n",
    "\n",
    "    query_passage_pairs = generate_query_passage_pairs(query, results)\n",
    "\n",
    "    if query_passage_pairs:\n",
    "        scores = reranker.compute_score(query_passage_pairs)\n",
    "        sorted_query_passage_pairs = sorted(zip(scores, query_passage_pairs), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        sorted_results = [(score, pair[1]) for score, pair in sorted_query_passage_pairs]\n",
    "\n",
    "        final_results = []\n",
    "        for score, passage in sorted_results:\n",
    "            # if score > 0:\n",
    "            final_results.append((score, passage))\n",
    "\n",
    "        final_results = final_results[:k]\n",
    "    else:\n",
    "        final_results = [(0.0, 'There is no relevant information for the given query!')]\n",
    "\n",
    "    all_segments = '\\n\\n'.join([result[1] for result in final_results])\n",
    "    class_found = final_syllabus[:-len('.txt')]\n",
    "\n",
    "    return f'Here are class information for {full_course_info[class_found]}:\\n{all_segments}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "WxaBEOcIw5gh"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def general_news_report(query):\n",
    "    \"\"\"\n",
    "    Fetches news articles based on the user's query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query for the news topics.\n",
    "        max_articles (int): Maximum number of articles to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted string containing the titles and descriptions of the retrieved articles.\n",
    "    \"\"\"\n",
    "    # Retrieve the News API key from user data or configuration\n",
    "    new_api_key = userdata.get('new_api')  # Ensure this variable is correctly set in your environment\n",
    "    max_articles=5\n",
    "    # News API endpoint\n",
    "    endpoint = 'https://newsapi.org/v2/everything'\n",
    "\n",
    "    # Set query parameters\n",
    "    params = {\n",
    "        'q': query,                # Use the user's query\n",
    "        'language': 'en',          # Language preference\n",
    "        'sortBy': 'publishedAt',   # Order by most recent\n",
    "        'apiKey': new_api_key,\n",
    "    }\n",
    "\n",
    "    # Send request to News API\n",
    "    response = requests.get(endpoint, params=params)\n",
    "\n",
    "    # Check for successful response\n",
    "    if response.status_code == 200:\n",
    "        news_data = response.json()\n",
    "        articles = news_data.get('articles', [])[:max_articles]  # Limit the number of articles\n",
    "        final_res = \"\"\n",
    "\n",
    "        # Parse and append each article's title and description\n",
    "        for article in articles:\n",
    "            title = article.get('title', 'No Title')\n",
    "            description = article.get('description', 'No Description')\n",
    "            final_res += f\"Title: {title}\\n\"\n",
    "            final_res += f\"Description: {description}\\n\\n\"\n",
    "\n",
    "        # Return the formatted string\n",
    "        return final_res.strip()\n",
    "\n",
    "    else:\n",
    "        # Handle errors and return the response status\n",
    "        error_message = f\"Failed to retrieve news: {response.status_code} - {response.reason}\"\n",
    "        print(error_message)\n",
    "        return error_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "ZO7sCKyoBaLn"
   },
   "outputs": [],
   "source": [
    "print(general_news_report(\"Los Angeles\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "TQw9Jv-xB4OW"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "def fetch_weather(params) -> str:\n",
    "    \"\"\"\n",
    "    Fetch weather information based on the provided city name.\n",
    "\n",
    "    Args:\n",
    "        params (str): A JSON-formatted string containing the city name and key selections.\n",
    "\n",
    "    Returns:\n",
    "        str: A stringified dictionary containing the selected weather details.\n",
    "    \"\"\"\n",
    "    # print(\"Step 1: Parsing input JSON...\")\n",
    "    # Parse the input JSON string to extract parameters\n",
    "    city_name = params\n",
    "\n",
    "    # Extract city name and key selections\n",
    "    # print(\"Step 2: Extracting city name and key selection...\")\n",
    "    key_selection = {\n",
    "            \"FeelsLikeC\": \"FeelsLikeC\",  # Feels like temperature in Celsius\n",
    "            \"temp_C\": \"temp_C\",         # Current temperature in Celsius\n",
    "            'weatherDesc': 'weatherDesc',\n",
    "            \"humidity\": \"humidity\"      # Humidity percentage\n",
    "        }\n",
    "\n",
    "    if not city_name:\n",
    "        # print(\"Error: 'city_name' is missing.\")\n",
    "        return \"Error: 'city_name' is required in the input parameters.\"\n",
    "\n",
    "    # print(f\"City name: {city_name}\")\n",
    "    # print(f\"Key selection: {key_selection}\")\n",
    "\n",
    "    # Make the API call to wttr.in\n",
    "    # print(\"Step 3: Making the API call...\")\n",
    "    try:\n",
    "        response = requests.get(f\"https://wttr.in/{city_name}?format=j1\", timeout=10)\n",
    "        # print(\"Step 4: API call complete. Status code:\", response.status_code)\n",
    "        response.raise_for_status()\n",
    "        weather_data = response.json()\n",
    "        # print(\"Step 5: API response JSON parsed successfully.\")\n",
    "    except requests.RequestException as e:\n",
    "        # print(\"Error: API call failed.\")\n",
    "        return f\"Error fetching weather data: {e}\"\n",
    "\n",
    "    # Extract the required weather information based on the key selection\n",
    "    # print(\"Step 6: Extracting weather information from API response...\")\n",
    "    try:\n",
    "        current_condition = weather_data.get(\"current_condition\", [{}])[0]\n",
    "        # print(\"Current condition data:\", current_condition)\n",
    "        result = {}\n",
    "\n",
    "        for display_key, json_key in key_selection.items():\n",
    "            # Extract the value from 'current_condition'\n",
    "            # print(f\"Extracting {display_key} using key '{json_key}'...\")\n",
    "            result[display_key] = current_condition.get(json_key, \"N/A\")\n",
    "\n",
    "        # print(\"Step 7: Weather information extraction complete.\")\n",
    "    except KeyError as e:\n",
    "        # print(\"Error: Key missing in the API response.\")\n",
    "        return f\"Error processing weather data: Key {e} not found in the response.\"\n",
    "\n",
    "    # Return the result as a string\n",
    "    # print(\"Step 8: Returning the result.\")\n",
    "    return str(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RPrfNQhUAVS_"
   },
   "outputs": [],
   "source": [
    "tools_list = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"RAG\",\n",
    "            \"description\": \"Retrieve the relevant section in the given knowledge base when the user asks information about courses or syllabus.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The original question that the user asks exactly, no need to rephrase.\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"query\"],\n",
    "                \"additionalProperties\": False,\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "     {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"fetch_weather\",\n",
    "                \"description\": \"Fetches the current weather for a specified city with user-defined key selections.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"params\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": (\n",
    "                                \"A string containing: \"\n",
    "                                \"the name of the city (e.g., 'London')\"\n",
    "                            ),\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"params\"],\n",
    "                    \"additionalProperties\": False,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"general_news_report\",\n",
    "                \"description\": \"Fetches recent news articles based on the user's query.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"query\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The user's search query for the news (e.g., 'artificial intelligence').\",\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"query\"],\n",
    "                    \"additionalProperties\": False,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sXc00S1WRyAB",
    "outputId": "afdd4a64-e44f-4e98-dd9f-5efd0b0f6584"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': '\\nYou have access to the following functions:\\n\\nUse the function \\'RAG\\' to \\'Retrieve the relevant section in the given knowledge base when the user asks information about courses or syllabus.\\'.\\nThe parameters are: {\"query\": {\"type\": \"string\", \"description\": \"The original question that the user asks exactly, no need to rephrase.\"}}, where [\\'query\\'] are required.\\n\\nUse the function \\'fetch_weather\\' to \\'Fetches the current weather for a specified city with user-defined key selections.\\':\\nThe parameters are: {\"params\": {\"type\": \"string\", \"description\": \"A string containing: the name of the city (e.g., \\'London\\')\"}}, where [\\'params\\'] are required.\\n\\nUse the function \\'general_news_report\\' to \\'Fetches recent news articles based on the user\\'s query.\\':\\nThe parameters are: {\"query\": {\"type\": \"string\", \"description\": \"The user\\'s search query for the news (e.g., \\'artificial intelligence\\').\"}}, where [\\'query\\'] are required.\\n\\nIf you choose to call a function ONLY reply in the following format with no prefix or suffix:\\n\\n<function=example_function_name>{\"example_name\": \"example_value\"}</function>\\n\\nReminder:\\n- Function calls MUST follow the specified format, start with <function= and end with </function>\\n- Required parameters MUST be specified\\n- Only call one function at a time\\n- Put the entire function call reply on one line\\n- If there is no function call available, return nothing!\\n\\n'}, {'role': 'user', 'content': 'what is happening in Los Angeles?'}]\n"
     ]
    }
   ],
   "source": [
    "toolPrompt = f\"\"\"\n",
    "You have access to the following functions:\n",
    "\n",
    "Use the function '{tools_list[0]['function']['name']}' to '{tools_list[0]['function']['description']}'.\n",
    "The parameters are: {json.dumps(tools_list[0]['function']['parameters']['properties'])}, where {tools_list[0]['function']['parameters']['required']} are required.\n",
    "\n",
    "Use the function '{tools_list[1]['function']['name']}' to '{tools_list[1]['function']['description']}':\n",
    "The parameters are: {json.dumps(tools_list[1]['function']['parameters']['properties'])}, where {tools_list[1]['function']['parameters']['required']} are required.\n",
    "\n",
    "Use the function '{tools_list[2]['function']['name']}' to '{tools_list[2]['function']['description']}':\n",
    "The parameters are: {json.dumps(tools_list[2]['function']['parameters']['properties'])}, where {tools_list[2]['function']['parameters']['required']} are required.\n",
    "\n",
    "If you choose to call a function ONLY reply in the following format with no prefix or suffix:\n",
    "\n",
    "<function=example_function_name>{{\\\"example_name\\\": \\\"example_value\\\"}}</function>\n",
    "\n",
    "Reminder:\n",
    "- Function calls MUST follow the specified format, start with <function= and end with </function>\n",
    "- Required parameters MUST be specified\n",
    "- Only call one function at a time\n",
    "- Put the entire function call reply on one line\n",
    "- If there is no function call available, return nothing!\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "  \t{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": toolPrompt,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": query,\n",
    "    },\n",
    "\n",
    "]\n",
    "\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Eu2rxD39dK6N"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"NousResearch/Hermes-2-Pro-Llama-3-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55yjn5RPhcWc",
    "outputId": "8611295f-f659-481b-870c-739ecc25816b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tool_call>\n",
      "{\"name\": \"general_news_report\", \"arguments\": {\"query\": \"Los Angeles news\"}}    \n",
      "</function><|im_end|>\n"
     ]
    }
   ],
   "source": [
    "def generate_tool_answer(input):\n",
    "    inputs = tokenizer.apply_chat_template(input, tools=tools_list, return_dict=True, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    out = model.generate(**inputs, max_new_tokens=1024, pad_token_id=tokenizer.eos_token_id)\n",
    "    res = tokenizer.decode(out[0][len(inputs[\"input_ids\"][0]):])\n",
    "    return res\n",
    "\n",
    "res = generate_tool_answer(messages)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zkxr9essn7GP",
    "outputId": "efd137c8-202d-4ea5-c945-9868e60ec553"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "general_news_report\n",
      "{'query': 'Los Angeles news'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def parse_tool_call(response_str: str):\n",
    "    pattern = r\">(.*?)</\"\n",
    "    match = re.search(pattern, response_str, flags=re.DOTALL)\n",
    "    if not match:\n",
    "        return None\n",
    "\n",
    "    tool_call_str = match.group(1).strip()\n",
    "\n",
    "    try:\n",
    "        tool_call_data = json.loads(tool_call_str)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "    function_name = tool_call_data.get(\"name\")\n",
    "    arguments = tool_call_data.get(\"arguments\", {})\n",
    "\n",
    "    return {\n",
    "        \"function\": function_name,\n",
    "        \"arguments\": arguments\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "parsed_response = parse_tool_call(res)\n",
    "print(parsed_response[\"function\"])\n",
    "print(parsed_response[\"arguments\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "5I-oPPklbUij"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "def generate_answer(input):\n",
    "    inputs = tokenizer.apply_chat_template(input, return_dict=True, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    out = model.generate(**inputs, max_new_tokens=1024, pad_token_id=tokenizer.eos_token_id)\n",
    "    return tokenizer.decode(out[0][len(inputs[\"input_ids\"][0]):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "zbHxMVQ-W1qP"
   },
   "outputs": [],
   "source": [
    "def parse_answer(answer: str):\n",
    "    cleaned = re.sub(r\"\\s*## Answer:\\s*\", \"\", answer)\n",
    "    cleaned = cleaned.replace(\"<|im_end|>\", \"\")\n",
    "    return cleaned.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NruQN-6VCwV9",
    "outputId": "e992305a-e72c-44d0-8497-3db3032e3029"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer from the LLM: \n",
      " In Los Angeles, several events are happening:\n",
      "\n",
      "1. Mercedes-Benz is developing a \"solar paint\" technology that could revolutionize electric vehicle charging. This innovative solar module is a thin, wafer-like coating that can be applied to the bodywork of electric cars, allowing them to harness solar energy more efficiently.\n",
      "\n",
      "2. The Kansas City Chiefs have won the AFC West for the 9th straight season, thanks to a game-winning field goal by Matthew Wright against the Los Angeles Chargers.\n",
      "\n",
      "3. Los Angeles Rams wide receiver Puka Nacua showcased remarkable skills with 178 scrimmage yards, two touchdowns, and two incredible catches in their victory over the Buffalo Bills.\n",
      "\n",
      "These events demonstrate the diverse range of activities and innovations taking place in Los Angeles.\n"
     ]
    }
   ],
   "source": [
    "if parsed_response:\n",
    "    available_functions = {\n",
    "        \"RAG\": RAG,\n",
    "        \"fetch_weather\": fetch_weather,\n",
    "        \"general_news_report\": general_news_report,\n",
    "    }\n",
    "\n",
    "    if parsed_response[\"function\"] not in available_functions:\n",
    "        available_function_names = \"\\n\".join(available_functions.keys())\n",
    "        raise NotImplementedError(\n",
    "            f\"Function {parsed_response['function']} is not implemented. \"\n",
    "            f\"Our available functions are:\\n\\n{available_function_names}\"\n",
    "        )\n",
    "\n",
    "    arguments = parsed_response[\"arguments\"]\n",
    "    function_to_call = available_functions[parsed_response[\"function\"]]\n",
    "    # print(function_to_call)\n",
    "    # print(arguments)\n",
    "    # print()\n",
    "\n",
    "    result = function_to_call(**arguments)\n",
    "    # print(\"function call result:\", result)\n",
    "    # print()\n",
    "\n",
    "    answer_prompt = \"You are a very helpful assistant. Please answer user's question according to given information. Trust the given information, it is completely align with the user's question.\"\n",
    "\n",
    "    new_messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": answer_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "                        ## Question:\n",
    "                        {query}\n",
    "\n",
    "                        ## Information:\n",
    "                        {result}\n",
    "                        \"\"\"\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # print(new_messages)\n",
    "    # print(\"-----------------------\")\n",
    "\n",
    "    res = generate_answer(new_messages)\n",
    "    parsed_res = parse_answer(res)\n",
    "    print(\"Answer from the LLM: \\n\", parsed_res)\n",
    "else:\n",
    "    print(\"No function call found in the response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "1NliEWQeIQp7"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    previous_RAG_info = None  # Placeholder for RAG information from prior turns\n",
    "    conversation_count = 0\n",
    "    chat_history = []  # To store the latest 10 conversations\n",
    "\n",
    "    while True:\n",
    "        # Get user query\n",
    "        user_query = input(\"User: \")\n",
    "        if user_query.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Exiting conversation.\")\n",
    "            break\n",
    "        use_previous_RAG = False\n",
    "        # Decide if previous RAG info should be used (dummy condition for now)\n",
    "        if conversation_count > 0 and previous_RAG_info:\n",
    "            use_previous_RAG = False  # Replace with actual condition\n",
    "        else:\n",
    "            use_previous_RAG = False\n",
    "\n",
    "        if use_previous_RAG:\n",
    "            user_query = f\"{user_query}\\n\\nAdditional context:\\n{previous_RAG_info}\"\n",
    "\n",
    "        # Add the new user query to the chat history\n",
    "        chat_history.append({\"role\": \"user\", \"content\": user_query})\n",
    "\n",
    "        # Ensure the chat history is limited to the latest 10 exchanges\n",
    "        if len(chat_history) > 20:  # Each \"exchange\" is a user+assistant pair\n",
    "            chat_history = chat_history[-20:]\n",
    "\n",
    "        # Construct messages with chat history\n",
    "        messages = [{\"role\": \"system\", \"content\": toolPrompt}] + chat_history\n",
    "\n",
    "        # Call the LLM to decide if tool usage needed\n",
    "        tool_response = generate_tool_answer(messages)\n",
    "        parsed_tool_response = parse_tool_call(tool_response)\n",
    "        # print(\"Tool:\", parsed_tool_response)\n",
    "\n",
    "        # Add assistant's response to chat history\n",
    "        if parsed_tool_response:\n",
    "            available_functions = {\n",
    "                \"RAG\": RAG,\n",
    "                \"fetch_weather\": fetch_weather,\n",
    "                \"general_news_report\": general_news_report,\n",
    "            }\n",
    "\n",
    "            function_name = parsed_tool_response[\"function\"]\n",
    "            arguments = parsed_tool_response[\"arguments\"]\n",
    "\n",
    "            if function_name in available_functions:\n",
    "                result = available_functions[function_name](**arguments)\n",
    "                if function_name == \"RAG\":\n",
    "                    previous_RAG_info = result  # Store RAG info for future turns\n",
    "            else:\n",
    "                print(f\"Unknown function: {function_name}\")\n",
    "\n",
    "            # Prepare the answer prompt for the second LLM call\n",
    "            answer_prompt = (\n",
    "                \"You are a very helpful assistant. Please answer the user's question using the information provided below. \"\n",
    "                \"Trust the given information, as it is aligned with the user's query.\\n\\n\"\n",
    "                f\"## Question:\\n{user_query}\\n\\n\"\n",
    "                f\"## Information:\\n{result}\"\n",
    "            )\n",
    "\n",
    "            # Create new LLM messages\n",
    "            new_messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "            new_messages += chat_history\n",
    "            new_messages.append({\"role\": \"user\", \"content\": answer_prompt})\n",
    "\n",
    "            # Call the LLM again with the tool's result\n",
    "            response = generate_answer(new_messages)\n",
    "            assistant_response = parse_answer(response)\n",
    "\n",
    "            print(\"Agent:\", assistant_response)\n",
    "            chat_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "        else:\n",
    "            messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "            messages += chat_history\n",
    "            messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "\n",
    "            # Call the LLM to generate a response\n",
    "            response = generate_answer(messages)\n",
    "            assistant_response = parse_answer(response)\n",
    "            print(\"Agent:\", assistant_response)\n",
    "            chat_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "        print()\n",
    "        print(\"**********split**********\")\n",
    "        print()\n",
    "\n",
    "        # Increment conversation count\n",
    "        conversation_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3yt4Ya1rIbgq",
    "outputId": "073254be-9703-43d8-f39d-e8694cb059d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: please remember this number 123\n",
      "Tool: None\n",
      "Agent: I will remember it. Is there anything else you would like me to remember or help you with?\n",
      "\n",
      "**********split**********\n",
      "\n",
      "User: what number did I type?\n",
      "Tool: None\n",
      "Agent: I'm happy to assist! You mentioned the number 123, is there anything else I can help you with? If you have a new question or need further help, please feel free to ask.\n",
      "\n",
      "**********split**********\n",
      "\n",
      "User: what is happening in Los Angeles?\n",
      "Tool: {'function': 'general_news_report', 'arguments': {'query': 'Los Angeles'}}\n",
      "Agent: Currently, several noteworthy events are happening in Los Angeles. Mercedes-Benz is researching a new type of solar module that could revolutionize electric vehicle charging. Additionally, the Kansas City Chiefs have won the AFC West for the 9th straight season. In other sports news, Rams WR Puka Nacua had an impressive performance, leading to the Tom Brady LFG Player of the Game. Lastly, the Mets have become the new kings of New York by signing Juan Soto to a record deal.\n",
      "\n",
      "**********split**********\n",
      "\n",
      "User: exit\n",
      "Exiting conversation.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
