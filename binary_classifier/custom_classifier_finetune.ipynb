{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "MG8EuOFhqRXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = '/content/train.csv'\n",
        "vali_file = '/content/vali.csv'\n",
        "test_file = '/content/test.csv'\n",
        "\n",
        "# Load datasets\n",
        "train_data = pd.read_csv(train_file)\n",
        "vali_data = pd.read_csv(vali_file)\n",
        "test_data = pd.read_csv(test_file)\n",
        "\n",
        "all_queries = pd.concat([\n",
        "    train_data['query1'], train_data['query2'],\n",
        "    vali_data['query1'], vali_data['query2'],\n",
        "    test_data['query1'], test_data['query2']\n",
        "])"
      ],
      "metadata": {
        "id": "BCJGLn4sqqcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_combined_embeddings(all_queries, individual_queries, embedding_dim=50):\n",
        "    \"\"\"\n",
        "    Train Word2Vec on combined queries and generate embeddings for individual queries.\n",
        "    \"\"\"\n",
        "    # Tokenize all queries\n",
        "    tokenized_all_queries = [query.split() for query in all_queries.tolist()]\n",
        "\n",
        "    # Train Word2Vec on the combined dataset\n",
        "    model = Word2Vec(\n",
        "        sentences=tokenized_all_queries,\n",
        "        vector_size=100,  # Larger embedding dimension for richer representations\n",
        "        min_count=5,      # Exclude very rare words\n",
        "        sg=1,             # Use Skip-Gram model\n",
        "        negative=10,      # Negative sampling for efficiency\n",
        "        epochs=10         # Train for more epochs\n",
        "    )\n",
        "\n",
        "    # Generate embeddings for the individual queries\n",
        "    embeddings = []\n",
        "    tokenized_individual_queries = [query.split() for query in individual_queries]\n",
        "    for sentence in tokenized_individual_queries:\n",
        "        word_vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
        "        if word_vectors:\n",
        "            avg_vector = np.mean(word_vectors, axis=0)\n",
        "        else:\n",
        "            avg_vector = np.zeros(embedding_dim)\n",
        "        embeddings.append(avg_vector)\n",
        "\n",
        "    return np.array(embeddings)\n",
        "\n",
        "# Generate embeddings for train, validation, and test sets\n",
        "train_query1_embeddings = generate_combined_embeddings(all_queries, train_data['query1'].tolist())\n",
        "train_query2_embeddings = generate_combined_embeddings(all_queries, train_data['query2'].tolist())\n",
        "train_embeddings = train_query1_embeddings + train_query2_embeddings\n",
        "\n",
        "vali_query1_embeddings = generate_combined_embeddings(all_queries, vali_data['query1'].tolist())\n",
        "vali_query2_embeddings = generate_combined_embeddings(all_queries, vali_data['query2'].tolist())\n",
        "vali_embeddings = vali_query1_embeddings + vali_query2_embeddings\n",
        "\n",
        "test_query1_embeddings = generate_combined_embeddings(all_queries, test_data['query1'].tolist())\n",
        "test_query2_embeddings = generate_combined_embeddings(all_queries, test_data['query2'].tolist())\n",
        "test_embeddings = test_query1_embeddings + test_query2_embeddings\n",
        "\n",
        "# Convert embeddings to tensors\n",
        "x_train = torch.tensor(train_embeddings, dtype=torch.float32)\n",
        "y_train = torch.tensor(train_data['label'].values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "x_vali = torch.tensor(vali_embeddings, dtype=torch.float32)\n",
        "y_vali = torch.tensor(vali_data['label'].values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "x_test = torch.tensor(test_embeddings, dtype=torch.float32)\n",
        "y_test = torch.tensor(test_data['label'].values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "print(\"Embedding generation complete!\")"
      ],
      "metadata": {
        "id": "ds4cZQCUqrxA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93389046-2c32-4936-a27d-6e93fe236042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding generation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYowFcq5qMTe",
        "outputId": "d20eebe0-817c-4591-8fe9-df2ffaa16dad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'hidden_dim': 64, 'lr': 0.01}, Validation Accuracy: 0.6560\n",
            "Test Accuracy: 0.5000\n",
            "Test Accuracy: 0.5300\n",
            "Test Accuracy: 0.5090\n",
            "Test Accuracy: 0.5050\n",
            "Test Accuracy: 0.5280\n",
            "Test Accuracy: 0.5730\n",
            "Test Accuracy: 0.6220\n",
            "Test Accuracy: 0.6230\n",
            "Test Accuracy: 0.6230\n",
            "Test Accuracy: 0.6290\n",
            "Epoch [10/100], Loss: 0.6654\n",
            "Test Accuracy: 0.6370\n",
            "Test Accuracy: 0.6390\n",
            "Test Accuracy: 0.6340\n",
            "Test Accuracy: 0.6360\n",
            "Test Accuracy: 0.6400\n",
            "Test Accuracy: 0.6430\n",
            "Test Accuracy: 0.6360\n",
            "Test Accuracy: 0.6380\n",
            "Test Accuracy: 0.6410\n",
            "Test Accuracy: 0.6500\n",
            "Epoch [20/100], Loss: 0.6241\n",
            "Test Accuracy: 0.6520\n",
            "Test Accuracy: 0.6550\n",
            "Test Accuracy: 0.6590\n",
            "Test Accuracy: 0.6640\n",
            "Test Accuracy: 0.6660\n",
            "Test Accuracy: 0.6690\n",
            "Test Accuracy: 0.6780\n",
            "Test Accuracy: 0.6760\n",
            "Test Accuracy: 0.6690\n",
            "Test Accuracy: 0.6800\n",
            "Epoch [30/100], Loss: 0.5936\n",
            "Test Accuracy: 0.6870\n",
            "Test Accuracy: 0.6720\n",
            "Test Accuracy: 0.6840\n",
            "Test Accuracy: 0.6920\n",
            "Test Accuracy: 0.6860\n",
            "Test Accuracy: 0.6930\n",
            "Test Accuracy: 0.6930\n",
            "Test Accuracy: 0.7000\n",
            "Test Accuracy: 0.6970\n",
            "Test Accuracy: 0.7040\n",
            "Epoch [40/100], Loss: 0.5696\n",
            "Test Accuracy: 0.7070\n",
            "Test Accuracy: 0.7040\n",
            "Test Accuracy: 0.7080\n",
            "Test Accuracy: 0.7070\n",
            "Test Accuracy: 0.7060\n",
            "Test Accuracy: 0.7070\n",
            "Test Accuracy: 0.7090\n",
            "Test Accuracy: 0.7120\n",
            "Test Accuracy: 0.7030\n",
            "Test Accuracy: 0.7040\n",
            "Epoch [50/100], Loss: 0.5583\n",
            "Test Accuracy: 0.7020\n",
            "Test Accuracy: 0.7040\n",
            "Test Accuracy: 0.7050\n",
            "Test Accuracy: 0.7070\n",
            "Test Accuracy: 0.7010\n",
            "Test Accuracy: 0.7070\n",
            "Test Accuracy: 0.7120\n",
            "Test Accuracy: 0.6990\n",
            "Test Accuracy: 0.7060\n",
            "Test Accuracy: 0.7050\n",
            "Epoch [60/100], Loss: 0.5417\n",
            "Test Accuracy: 0.7000\n",
            "Test Accuracy: 0.7020\n",
            "Test Accuracy: 0.7030\n",
            "Test Accuracy: 0.7030\n",
            "Test Accuracy: 0.7010\n",
            "Test Accuracy: 0.7010\n",
            "Test Accuracy: 0.7030\n",
            "Test Accuracy: 0.6980\n",
            "Test Accuracy: 0.6990\n",
            "Test Accuracy: 0.7000\n",
            "Epoch [70/100], Loss: 0.5307\n",
            "Test Accuracy: 0.7080\n",
            "Test Accuracy: 0.7090\n",
            "Test Accuracy: 0.7000\n",
            "Test Accuracy: 0.7090\n",
            "Test Accuracy: 0.7100\n",
            "Test Accuracy: 0.7060\n",
            "Test Accuracy: 0.7100\n",
            "Test Accuracy: 0.7080\n",
            "Test Accuracy: 0.7120\n",
            "Test Accuracy: 0.7110\n",
            "Epoch [80/100], Loss: 0.5200\n",
            "Test Accuracy: 0.7080\n",
            "Test Accuracy: 0.7110\n",
            "Test Accuracy: 0.7050\n",
            "Test Accuracy: 0.7120\n",
            "Test Accuracy: 0.7080\n",
            "Test Accuracy: 0.7070\n",
            "Test Accuracy: 0.7060\n",
            "Test Accuracy: 0.7070\n",
            "Test Accuracy: 0.7050\n",
            "Test Accuracy: 0.7030\n",
            "Epoch [90/100], Loss: 0.5093\n",
            "Test Accuracy: 0.7120\n",
            "Test Accuracy: 0.7030\n",
            "Test Accuracy: 0.7050\n",
            "Test Accuracy: 0.7110\n",
            "Test Accuracy: 0.7130\n",
            "Test Accuracy: 0.7060\n",
            "Test Accuracy: 0.7030\n",
            "Test Accuracy: 0.7050\n",
            "Test Accuracy: 0.7070\n",
            "Test Accuracy: 0.7080\n",
            "Epoch [100/100], Loss: 0.5004\n",
            "Final Accuracy: 0.7080\n"
          ]
        }
      ],
      "source": [
        "class BinaryClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(BinaryClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "best_accuracy = 0\n",
        "best_hyperparams = {}\n",
        "\n",
        "for hidden_dim in [32, 64, 128]:  # Try different hidden dimensions\n",
        "    for lr in [0.1, 0.01, 0.001]:     # Try different learning rates\n",
        "        model = BinaryClassifier(x_train.shape[1], hidden_dim)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "        # Training\n",
        "        for epoch in range(50):  # Train for fewer epochs to save time during tuning\n",
        "            model.train()\n",
        "            outputs = model(x_train)\n",
        "            loss = nn.BCELoss()(outputs, y_train)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            predictions = model(x_vali)\n",
        "            predicted_labels = (predictions > 0.5).float()\n",
        "            accuracy = (predicted_labels == y_vali).float().mean().item()\n",
        "\n",
        "        # Track the best hyperparameters\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_hyperparams = {'hidden_dim': hidden_dim, 'lr': lr}\n",
        "\n",
        "print(f\"Best Hyperparameters: {best_hyperparams}, Validation Accuracy: {best_accuracy:.4f}\")\n",
        "\n",
        "# Train final model with best hyperparameters\n",
        "model = BinaryClassifier(x_train.shape[1], best_hyperparams['hidden_dim'])\n",
        "optimizer = optim.Adam(model.parameters(), lr=best_hyperparams['lr'])\n",
        "\n",
        "# Train on the combined training and validation sets\n",
        "x_combined = torch.cat((x_train, x_vali), dim=0)\n",
        "y_combined = torch.cat((y_train, y_vali), dim=0)\n",
        "\n",
        "epochs = 100\n",
        "for epoch in range(epochs):  # Full training\n",
        "    model.train()\n",
        "    outputs = model(x_combined)\n",
        "    loss = nn.BCELoss()(outputs, y_combined)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Test the final model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(x_test)\n",
        "        predicted_labels = (predictions > 0.5).float()\n",
        "        accuracy = (predicted_labels == y_test).float().mean()\n",
        "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(f\"Final Accuracy: {accuracy:.4f}\")"
      ]
    }
  ]
}