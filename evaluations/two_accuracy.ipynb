{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2U9SR8Xfai3F",
    "outputId": "80b9990f-246e-4bfb-9920-907c0f67a582"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sat Nov 16 06:19:40 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   55C    P0              26W /  70W |   7037MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
      "Cuda compilation tools, release 12.2, V12.2.140\n",
      "Build cuda_12.2.r12.2/compiler.33191640_0\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7E2bQAuBai3G",
    "outputId": "921c2d8b-830f-4753-8871-c145a4b6a58b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: xformers in /usr/local/lib/python3.10/dist-packages (0.0.28.post3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.26.4)\n",
      "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from xformers) (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers) (2024.3.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->xformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->xformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->xformers) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q einops\n",
    "!pip install -q FlagEmbedding\n",
    "!pip install -q peft\n",
    "!pip install -q faiss-gpu\n",
    "!pip install xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJlntAEwai3H"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from numpy.linalg import norm\n",
    "from FlagEmbedding import FlagModel\n",
    "from FlagEmbedding import FlagReranker\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "courses_numbers = ['544', '566', '585', '596', '599', '626', '677', '699']\n",
    "course_names = ['Applied Natural Language Processing', 'Deep Learning and its Applications', 'Database Systems', 'Scientific Computing and Visualization',\n",
    "         'Distributed Systems', 'Text as Data', 'Advanced Computer Vision', 'Robotic Perception']\n",
    "\n",
    "name_to_num = dict(zip(course_names, courses_numbers))\n",
    "\n",
    "course_ranges = {\n",
    "    'csci544': './CSCI544.txt',\n",
    "    'csci566': './CSCI566.txt',\n",
    "    'csci585': './CSCI585.txt',\n",
    "    'csci596': './CSCI596.txt',\n",
    "    'csci599': './CSCI599.txt',\n",
    "    'csci626': './CSCI626.txt',\n",
    "    'csci677': './CSCI677.txt',\n",
    "    'csci699': './CSCI699.txt'\n",
    "}"
   ],
   "metadata": {
    "id": "sfFR5KQ8bft6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lp6MZ7eIai3H",
    "outputId": "c8129274-ef26-4419-a7dc-adda3178d607"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please select the embedding model to load:\n",
      "1: BGE + BGE Reranker\n",
      "2: Jina + Jina Reranker\n",
      "3: Stella + BGE Reranker\n",
      "4: Stella + Jina Reranker\n",
      "5: Roberta + BGE Reranker\n",
      "Enter a number (1-5): 1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_models(embed_model_name=None, rerank_model_name=None):\n",
    "\n",
    "    # 如果未提供模型名称，设置默认的嵌入模型和重排序模型名称\n",
    "    embed_model_name = embed_model_name or 'BAAI/bge-base-en-v1.5'\n",
    "    rerank_model_name = rerank_model_name or 'BAAI/bge-reranker-v2-m3'\n",
    "\n",
    "    # 加载嵌入模型\n",
    "    embed_model = AutoModel.from_pretrained(embed_model_name, trust_remote_code=True, torch_dtype=torch.float32)\n",
    "    embed_tokenizer = AutoTokenizer.from_pretrained(embed_model_name, trust_remote_code=True)\n",
    "    embed_model.to(device)\n",
    "    embed_model.eval()\n",
    "\n",
    "    # 加载重排序模型\n",
    "    rerank_model = AutoModelForSequenceClassification.from_pretrained(rerank_model_name, torch_dtype='auto', trust_remote_code=True)\n",
    "    rerank_tokenizer = AutoTokenizer.from_pretrained(rerank_model_name)\n",
    "    rerank_model.to(device)\n",
    "    rerank_model.eval()\n",
    "\n",
    "    return embed_model, embed_tokenizer, rerank_model, rerank_tokenizer\n",
    "\n",
    "# 定义可用的嵌入模型字典\n",
    "available_embed_models = {\n",
    "    '1': 'BAAI/bge-base-en-v1.5',                      # BGE\n",
    "    '2': 'jinaai/jina-embeddings-v2-base-en',          # Jina\n",
    "    '3': 'dunzhang/stella_en_400M_v5',                 # Stella + BGE Reranker\n",
    "    '4': 'dunzhang/stella_en_400M_v5',                 # Stella + Jina Reranker\n",
    "    '5': 'sentence-transformers/all-roberta-large-v1'  # Roberta\n",
    "}\n",
    "\n",
    "# 定义可用的重排序模型字典\n",
    "available_rerank_models = {\n",
    "    '1': 'BAAI/bge-reranker-v2-m3',                             # BGE Reranker\n",
    "    '2': 'jinaai/jina-reranker-v2-base-multilingual',           # Jina Reranker\n",
    "    '3': 'BAAI/bge-reranker-v2-m3',                             # Stella + BGE Reranker\n",
    "    '4': 'jinaai/jina-reranker-v2-base-multilingual',           # Stella + Jina Reranker\n",
    "    '5': 'BAAI/bge-reranker-v2-m3'                              # Roberta + BGE Reranker\n",
    "}\n",
    "\n",
    "print(\"Please select the embedding model to load:\")\n",
    "print(\"1: BGE + BGE Reranker\")\n",
    "print(\"2: Jina + Jina Reranker\")\n",
    "print(\"3: Stella + BGE Reranker\")\n",
    "print(\"4: Stella + Jina Reranker\")\n",
    "print(\"5: Roberta + BGE Reranker\")\n",
    "\n",
    "user_choice = input(\"Enter a number (1-5): \").strip()\n",
    "\n",
    "# 获取用户选择的嵌入模型和对应的重排序模型\n",
    "selected_embed_model = available_embed_models.get(user_choice)\n",
    "selected_rerank_model = available_rerank_models.get(user_choice)\n",
    "\n",
    "# 加载选择的模型\n",
    "embed_model, embed_tokenizer, rerank_model, rerank_tokenizer = load_models(embed_model_name=selected_embed_model, rerank_model_name=selected_rerank_model)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# 对文本进行编码的函数\n",
    "def encode(texts):\n",
    "    inputs = embed_tokenizer(texts, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = embed_model(**inputs).last_hidden_state.mean(dim=1)\n",
    "        # 转换为float32\n",
    "    return embeddings.to(torch.float32)\n",
    "\n",
    "# 读取txt文件并按\"\\n\\n\"切割 -> 自行修改！！！\n",
    "def read_and_split_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    segments = content.split('\\n\\n##')\n",
    "\n",
    "    return segments\n",
    "\n",
    "# 计算query和每个文档分段的相似度，并返回排序后的结果\n",
    "def compute_similarity(query, segments):\n",
    "    query_embedding = encode([query])[0].cpu()\n",
    "    segment_embeddings = encode(segments).cpu()\n",
    "\n",
    "    # 计算余弦相似度的函数\n",
    "    cos_sim = lambda a, b: (a @ b.T if b.ndim > 1 else a @ b) / (norm(a) * norm(b))\n",
    "\n",
    "    similarities = [cos_sim(query_embedding, segment_embedding) for segment_embedding in segment_embeddings]\n",
    "\n",
    "    # 将分数、query和文档分段组合并排序\n",
    "    results = sorted(zip(similarities, segments), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    return results\n",
    "\n",
    "def generate_embeddings(texts):\n",
    "    # 使用 embed_tokenizer 和 embed_model 生成嵌入（适用于 BGE、Stella 等模型）\n",
    "    inputs = embed_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        embeddings = embed_model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy().astype('float32')\n",
    "    return embeddings\n",
    "\n",
    "def build_vector_database(course_names, user_choice):\n",
    "    print(\"Starting to build vector database...\")\n",
    "\n",
    "    if user_choice == '2':  # 如果选择的是 Jina 模型\n",
    "        print(\"Using Jina model to encode course names...\")\n",
    "        course_embeddings = embed_model.encode(course_names)  # 直接使用 embed_model.encode 方法生成嵌入\n",
    "    else:  # 如果是 BGE、Stella 等其他模型\n",
    "        print(\"Using generate_embeddings to encode course names...\")\n",
    "        course_embeddings = generate_embeddings(course_names)  # 调用 generate_embeddings 生成嵌入\n",
    "\n",
    "    # 转为 float32 并进行 L2 归一化\n",
    "    np_course_embeddings = np.array(course_embeddings).astype('float32')\n",
    "    faiss.normalize_L2(np_course_embeddings)\n",
    "\n",
    "\n",
    "    # 创建 FAISS 索引，选择内积搜索方式 (dot-product search)\n",
    "    index_innerproduct = faiss.IndexFlatIP(np_course_embeddings.shape[1])\n",
    "    print(\"FAISS index created on CPU.\")\n",
    "\n",
    "    # 将已归一化的嵌入添加到 CPU 索引中\n",
    "    #index_innerproduct.add(np_course_embeddings)\n",
    "    #print(\"Embeddings added to CPU index.\")\n",
    "\n",
    "    #return index_innerproduct  # 返回 CPU 上的索引##\n",
    "\n",
    "    ## 将索引从 CPU 移动到 GPU\n",
    "    res = faiss.StandardGpuResources()  # 创建 GPU 资源\n",
    "    gpu_index = faiss.index_cpu_to_gpu(res, 0, index_innerproduct)  # 将索引移动到 GPU\n",
    "\n",
    "    # 将已归一化的嵌入添加到 GPU 索引中\n",
    "    gpu_index.add(np_course_embeddings)\n",
    "    print(\"Embeddings added to GPU index.\")\n",
    "\n",
    "    return gpu_index  # 返回 GPU 上的索引\n",
    "\n",
    "database = build_vector_database(course_names, user_choice)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jBCQAdGDcNpg",
    "outputId": "213dbb2d-c1e2-4d4a-f543-e142bd037462"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting to build vector database...\n",
      "Using generate_embeddings to encode course names...\n",
      "FAISS index created on CPU.\n",
      "Embeddings added to GPU index.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUaybVNWai3I"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def load_queries_from_csv(file_path):\n",
    "    queries = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            if len(row) < 6:\n",
    "                print(f\"Skipping incomplete row: {row}\")\n",
    "                continue\n",
    "\n",
    "            if any(cell.strip().lower() in [\"n/a\", \"nah\"] for cell in row):\n",
    "                #print(f\"Skipping invalid question row: {row}\")\n",
    "                continue\n",
    "\n",
    "            # 提取有效的列数据\n",
    "            query = row[0].strip()\n",
    "            original_query = row[1].strip()\n",
    "            course_class = row[2].strip()\n",
    "            most_relevant_segment = row[3].strip()\n",
    "            respond = row[4].strip()\n",
    "            function_to_call = row[5].strip()\n",
    "            queries.append((query, original_query, course_class, most_relevant_segment, respond, function_to_call))\n",
    "\n",
    "    return queries\n",
    "\n",
    "def extract_first_line(segment):\n",
    "    # 检查段落是否包含 `##`，并提取 `##` 后面的第一行内容\n",
    "    lines = segment.strip().split('\\n')\n",
    "    if lines[0].startswith(\"##\"):\n",
    "        return lines[1].strip() if len(lines) > 1 else \"\"  # 提取 `##` 后的第一行内容\n",
    "    else:\n",
    "        return lines[0].strip()  # 没有 `##`，返回整个段落的第一行\n",
    "\n",
    "\n",
    "# 执行检索并获取 top-k 结果\n",
    "def RAG(queries, segments, k=3):\n",
    "    retrieved_segments = []\n",
    "\n",
    "    for idx, query_tuple in enumerate(queries, 1):  # 使用 enumerate 来获取索引\n",
    "        query = query_tuple[0]  # 使用短的 query 作为检索项\n",
    "        original_query = query_tuple[1]  # 获取 CSV 中的实际查询内容\n",
    "        most_relevant_segment = query_tuple[3]\n",
    "        all_similarity = compute_similarity(query, segments)\n",
    "\n",
    "        # 按相似度筛选段落\n",
    "        results = []\n",
    "        for score, segment in all_similarity:\n",
    "            if score > -5:\n",
    "                results.append(segment)\n",
    "\n",
    "        # 如果有足够相关的段落，将其重新排序并获取前 k 个\n",
    "        if results:\n",
    "            sentence_pairs = [[query, doc] for doc in results]\n",
    "\n",
    "            final_results = []\n",
    "            for pair in sentence_pairs:\n",
    "                inputs = rerank_tokenizer(pair[0], pair[1], return_tensors='pt', truncation=True, max_length=1024).to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = rerank_model(**inputs)\n",
    "                    score = outputs.logits.cpu().float().numpy().flatten()[0]\n",
    "\n",
    "                    final_results.append((score, pair[1]))\n",
    "\n",
    "            # 排序并选择 top k\n",
    "            final_results.sort(key=lambda x: x[0], reverse=True)\n",
    "            top_k_results = [extract_first_line(pair[1]) for pair in final_results[:k]]\n",
    "        else:\n",
    "            top_k_results = ['!!!Failed to find any segment!!!']\n",
    "\n",
    "        # 打印实际查询内容和前 k 个结果\n",
    "        print(f\"Query {idx} ({original_query}): Top {k} Segments: {top_k_results}\")\n",
    "        print(f\"Expected Segment from CSV: {most_relevant_segment}\")\n",
    "\n",
    "        # 将前 k 个结果存储到 retrieved_segments 列表中\n",
    "        retrieved_segments.append(top_k_results)\n",
    "\n",
    "    return retrieved_segments\n",
    "\n",
    "# 搜索最相似的课程\n",
    "def search(query, database, embed_dim, topk=1):\n",
    "    # 判断使用哪种方法生成查询嵌入\n",
    "    if user_choice == '2':  # Jina 模型\n",
    "        query_embed = embed_model.encode([query])[0]\n",
    "    else:  # 其他模型，如 BGE、Stella 等\n",
    "        query_embed = generate_embeddings([query])[0]\n",
    "\n",
    "    query_embed = np.array(query_embed).astype('float32')\n",
    "\n",
    "    _, idx = database.search(query_embed.reshape((1, embed_dim)), topk)  # (768,) -> (1, 768)\n",
    "    idx = idx.reshape(-1)\n",
    "\n",
    "    ret = [course_names[i] for i in idx]\n",
    "    return ret\n",
    "\n",
    "def get_top_3_answers(query, course_segments):\n",
    "    # Compute similarity for the query against course segments\n",
    "    all_similarity = compute_similarity(query, course_segments)\n",
    "\n",
    "    # Filter results by a threshold\n",
    "    results = [segment for score, segment in all_similarity if score > 0.25]\n",
    "    final_results = []\n",
    "\n",
    "    if results:\n",
    "        # Prepare sentence pairs for reranking\n",
    "        sentence_pairs = [[query, doc] for doc in results]\n",
    "\n",
    "        # Rerank and score each pair\n",
    "        for pair in sentence_pairs:\n",
    "            inputs = rerank_tokenizer(pair[0], pair[1], return_tensors='pt', truncation=True, max_length=1024).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = rerank_model(**inputs)\n",
    "                score = outputs.logits.cpu().float().numpy().flatten()[0]\n",
    "                final_results.append((score, pair[1]))\n",
    "\n",
    "        # Sort and select the top 3 results\n",
    "        final_results.sort(key=lambda x: x[0], reverse=True)\n",
    "        final_results = final_results[:3]\n",
    "    else:\n",
    "        final_results = [(0.0, '!!!Failed to find any segment!!!')]\n",
    "\n",
    "    # Print top 3 answers with scores\n",
    "    for i, (score, answer) in enumerate(final_results, 1):\n",
    "        print(f\"Top {i} Answer: {answer}\\nScore: {score:.4f}\\n\")\n",
    "\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qjJp1_aBai3I"
   },
   "outputs": [],
   "source": [
    "queries1 = load_queries_from_csv('./combined_data.csv')\n",
    "queries2 = load_queries_from_csv('./combined_fake_data.csv')\n",
    "queries = [q1[0] for q1 in queries1] + [q2[0] for q2 in queries2]\n",
    "courses = [q1[2] for q1 in queries1] + [q2[2] for q2 in queries2]\n",
    "\n",
    "most_relevant_segment = [q1[3] for q1 in queries1]\n",
    "ground_truth = [q1[4] for q1 in queries1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZW9ghCx5ai3I",
    "outputId": "8a43bd2a-09bb-4528-88a6-b87b932515f3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total(除去包含课号课名的query数量): 75\n",
      "Accuracy: 98.67%\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "def evaluate_find_class(query: List[str], ground_truth: List[str], database) -> float:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    query: a list of all the user's queries -> [query_1, query_2, ..., query_n]\n",
    "    ground_truth: a list of ground truth (classes) -> [class_1, class_2, ..., class_n]\n",
    "    database: vector database\n",
    "\n",
    "    Return:\n",
    "    accuracy\n",
    "    \"\"\"\n",
    "    def course_in_query(query):\n",
    "        for num in courses_numbers:\n",
    "            if f\"csci{num}\" in query.lower() or num in query:\n",
    "                return True\n",
    "\n",
    "        for name in course_names:\n",
    "            if name.lower() in query.lower():\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for q, gt in zip(query, ground_truth):\n",
    "        if not course_in_query(q):\n",
    "            total += 1\n",
    "\n",
    "            course_result = search(q, database, embed_dim=database.d, topk=1)[0]\n",
    "            course_num_result = name_to_num[course_result]\n",
    "            if gt in f\"csci{course_num_result}\":\n",
    "                correct += 1\n",
    "\n",
    "    accuracy = (correct / total) * 100\n",
    "\n",
    "    print(f\"Total(除去包含课号课名的query数量): {total}\")\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "accuracy = evaluate_find_class(queries, courses, database)\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MU-9nFuABi2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a72f7474-d4ca-4318-d8fe-7ddcfa5a58c2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Top-1 Accuracy: 95.35%\n",
      "Top-3 Accuracy: 95.35%\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def evaluate_rag(retrieved_segments: List[List[str]], ground_truth: List[str], k: int = 3) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    retrieved_segments: a list where each element is a list of the top-k segments retrieved by RAG -> [[seg_1, ..., seg_k], [seg_1, ..., seg_k], ..., [seg_1, ..., seg_k]]\n",
    "    ground_truth: a list of the most relevant ground truth segment for each query -> [seg_gt_1, seg_gt_2, ..., seg_gt_n]\n",
    "    k: the number of top segments retrieved for each query (default is 3)\n",
    "\n",
    "    Return:\n",
    "    top_1 accuracy and top_k accuracy\n",
    "    \"\"\"\n",
    "    top_1_correct = 0\n",
    "    top_k_correct = 0\n",
    "    total = len(retrieved_segments)\n",
    "\n",
    "    for segments, gt in zip(retrieved_segments, ground_truth):\n",
    "        all_similarity = compute_similarity(gt, [segments])\n",
    "        top_k_scores = [score for score, _ in all_similarity]\n",
    "\n",
    "        if top_k_scores[0] > 0.70:\n",
    "            top_1_correct += 1\n",
    "        # else:\n",
    "        #     print(f\"===================> segments: {segments}\")\n",
    "        #     print(f\"===================> gt: {gt}\")\n",
    "        #     print(f\"===================> top_k_scores: {top_k_scores[0]}\")\n",
    "\n",
    "        for s in top_k_scores:\n",
    "            if s > 0.70:\n",
    "                top_k_correct += 1\n",
    "                break\n",
    "\n",
    "    top_1_accuracy = (top_1_correct / total) * 100\n",
    "    top_k_accuracy = (top_k_correct / total) * 100\n",
    "\n",
    "    return top_1_accuracy, top_k_accuracy\n",
    "\n",
    "top_1_acc, top_k_acc = evaluate_rag(most_relevant_segment, ground_truth, k=1)\n",
    "print(f\"Top-1 Accuracy: {top_1_acc:.2f}%\")\n",
    "print(f\"Top-3 Accuracy: {top_k_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oF9ovwMuai3J"
   },
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1r-fcg1ai3J",
    "outputId": "679b1f70-70e8-4a4d-e494-8e1019f88db6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='10', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)\n",
      "Relevance and Correctness Score: 10\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    # api_key=\"YOUR_API_KEY\"\n",
    ")\n",
    "\n",
    "def evaluate_rag_answer(rag_answer: str, reference_answer: str, question: str) -> int:\n",
    "    \"\"\"\n",
    "    Evaluates the relevance and correctness of a RAG model's answer by prompting ChatGPT-4 and returns a score.\n",
    "\n",
    "    Parameters:\n",
    "    rag_answer (str): The answer generated by the RAG model.\n",
    "    reference_answer (str): The expected or reference answer for comparison.\n",
    "    question (str): The question that was asked to generate the answer.\n",
    "\n",
    "    Returns:\n",
    "    int: A relevance and correctness score between 1 and 10.\n",
    "    \"\"\"\n",
    "    # Compose the prompt for ChatGPT-4\n",
    "    prompt = (\n",
    "        f\"Evaluate the relevance and correctness of the following answer generated by a RAG model.\\n\\n\"\n",
    "        f\"Question: {question}\\n\\n\"\n",
    "        f\"RAG Model Answer: {rag_answer}\\n\\n\"\n",
    "        f\"Reference Answer: {reference_answer}\\n\\n\"\n",
    "        \"Please provide a single relevance and correctness score between 1 and 10, where 10 indicates the highest relevance and correctness, \"\n",
    "        \"and 1 indicates no relevance or correctness. Return only a number\"\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=50\n",
    "    )\n",
    "\n",
    "    print(response.choices[0].message)\n",
    "\n",
    "    try:\n",
    "        relevance_score = int(response.choices[0].message.content.strip())\n",
    "    except ValueError:\n",
    "        # In case parsing fails, return a default score or indicate error\n",
    "        relevance_score = None\n",
    "\n",
    "    return relevance_score\n",
    "\n",
    "# Sample usage\n",
    "rag_answer = \"The Eiffel Tower is located in Paris, France.\"\n",
    "reference_answer = \"The Eiffel Tower is in Paris.\"\n",
    "question = \"Where is the Eiffel Tower located?\"\n",
    "\n",
    "score = evaluate_rag_answer(rag_answer, reference_answer, question)\n",
    "print(\"Relevance and Correctness Score:\", score)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
