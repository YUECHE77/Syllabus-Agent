{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GEqGQS0hhwHg",
        "outputId": "29db6965-9377-4430-ced4-b01e1b8d12a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.27.8 in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (4.66.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (3.11.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.8) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.8) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.8) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.8) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.27.8) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q together\n",
        "!pip install -q FlagEmbedding\n",
        "!pip install -q peft\n",
        "!pip install -q faiss-gpu\n",
        "!pip install openai==0.27.8"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from FlagEmbedding import FlagModel\n",
        "from FlagEmbedding import FlagReranker\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import faiss\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "import openai\n",
        "from together import Together"
      ],
      "metadata": {
        "id": "FMhkAu2Ki0jD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "TOGETHER_API_KEY = userdata.get('TOGETHER_API_KEY')\n",
        "together = Together(api_key=TOGETHER_API_KEY)\n",
        "\n",
        "openai.api_key = userdata.get('OpenAI_API_Key')"
      ],
      "metadata": {
        "id": "ceeQG_ZGjI6t"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_query_and_rag(data_path):\n",
        "    content = pd.read_csv(data_path)\n",
        "\n",
        "    queries = content['query'].tolist()\n",
        "    rag_sections = content['RAG_retrieval_results'].tolist()\n",
        "    reference_answers = content['reference_answer'].tolist()\n",
        "\n",
        "    assert len(queries) == len(rag_sections) == len(reference_answers)\n",
        "\n",
        "    return queries, rag_sections, reference_answers\n",
        "\n",
        "data_path = '/content/All_RAG_results.csv'\n",
        "queries, rag_sections, reference_answers = get_query_and_rag(data_path)"
      ],
      "metadata": {
        "id": "oWbOcHH2jw4S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Qwen"
      ],
      "metadata": {
        "id": "hI2TobMFqBa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Qwen_all_response = []\n",
        "\n",
        "answer_prompt = \"You are a very helpful assistant. Please answer user's question according to given information. Trust the given information, it is completely align with the user's question.\"\n",
        "\n",
        "for query, rag in tqdm(zip(queries, rag_sections)):\n",
        "    new_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": answer_prompt,\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"\n",
        "## Question:\n",
        "{query}\n",
        "\n",
        "## Information:\n",
        "{rag}\n",
        "\"\"\"\n",
        "    },\n",
        "]\n",
        "    qwen_res = together.chat.completions.create(\n",
        "        model=\"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
        "        messages=new_messages,\n",
        "        max_tokens=1000,\n",
        "        temperature=0.9,\n",
        "    )\n",
        "\n",
        "    Qwen_all_response.append(qwen_res.choices[0].message.content)"
      ],
      "metadata": {
        "id": "4_95W2lRkcSz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "712921cf-bd31-45cd-83d7-bb652bd6244f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "305it [07:12,  1.42s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qwen_csv_dict = {'query': queries, 'reference_answer': reference_answers, 'RAG_retrieval_results': rag_sections, 'Agent_responds': Qwen_all_response}\n",
        "df = pd.DataFrame(qwen_csv_dict)\n",
        "df.to_csv('/content/Qwen_responds.csv', index=False)"
      ],
      "metadata": {
        "id": "FzLgzohwqcVz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. GPT-4"
      ],
      "metadata": {
        "id": "LubTOIurtMH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_4_all_response = []\n",
        "\n",
        "for query, rag in tqdm(zip(queries, rag_sections)):\n",
        "    new_messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": answer_prompt,\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"\n",
        "## Question:\n",
        "{query}\n",
        "\n",
        "## Information:\n",
        "{rag}\n",
        "\"\"\"\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    gpt_response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",  # Specify the ChatGPT-4 model\n",
        "        messages=new_messages,\n",
        "        max_tokens=1024,\n",
        "    )\n",
        "\n",
        "    GPT_4_all_response.append(gpt_response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYDF0TSVtN9i",
        "outputId": "64d35c46-d6ac-464e-c474-6c322b912fd6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "305it [10:31,  2.07s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_csv_dict = {'query': queries, 'reference_answer': reference_answers, 'RAG_retrieval_results': rag_sections, 'Agent_responds': GPT_4_all_response}\n",
        "df_gpt = pd.DataFrame(gpt_csv_dict)\n",
        "df_gpt.to_csv('/content/GPT_responds.csv', index=False)"
      ],
      "metadata": {
        "id": "9yFD7-entoTh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. llama"
      ],
      "metadata": {
        "id": "iwFLEfiE-ZSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llama_all_response = []\n",
        "\n",
        "for query, rag in tqdm(zip(queries, rag_sections)):\n",
        "    new_messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": answer_prompt,\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"\n",
        "## Question:\n",
        "{query}\n",
        "\n",
        "## Information:\n",
        "{rag}\n",
        "\"\"\"\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    llama_res = together.chat.completions.create(\n",
        "        model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "        messages=new_messages,\n",
        "        max_tokens=1000,\n",
        "        temperature=0.9,\n",
        "    )\n",
        "\n",
        "    llama_all_response.append(llama_res.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVHIMCXgxL0k",
        "outputId": "1ae991c3-d157-41d3-cbf2-ca9f13bebfcf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "305it [06:54,  1.36s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llama_csv_dict = {'query': queries, 'reference_answer': reference_answers, 'RAG_retrieval_results': rag_sections, 'Agent_responds': llama_all_response}\n",
        "df_llama = pd.DataFrame(llama_csv_dict)\n",
        "df_llama.to_csv('/content/llama_responds.csv', index=False)"
      ],
      "metadata": {
        "id": "WT9On2tq-_aq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RKDBL_QOBJ9u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}