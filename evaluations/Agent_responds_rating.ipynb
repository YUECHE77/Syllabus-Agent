{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "executionInfo": {
     "elapsed": 14962,
     "status": "error",
     "timestamp": 1732829049146,
     "user": {
      "displayName": "marin jiang",
      "userId": "14123898113798451330"
     },
     "user_tz": 480
    },
    "id": "UIZgA4rREUvU",
    "outputId": "3a8a3cea-8603-4f38-b171-69f71024d125"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import re\n",
    "\n",
    "client = OpenAI(api_key=\"YOUR_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qIsEEBAwHL-P",
    "outputId": "651c8cbd-e938-41ae-ddab-fb05afb230c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Evaluate the relevance and correctness of the answers generated by our agent based on the information retrieved by the RAG model. The evaluation will consider the following details:\n",
      "\n",
      "        1. User's Query:\n",
      "        {query}\n",
      "        \n",
      "        2. Information retrieved by the RAG model:\n",
      "        {rag_section}\n",
      "        \n",
      "        3. Agent's Answer:\n",
      "        {agent_answer}\n",
      "        \n",
      "        4. Reference Answer (generated by humans):\n",
      "        {reference_answer}\n",
      "        \n",
      "        **Rating Guidelines:**\n",
      "        \n",
      "        Rate the agent's answer on a scale of 1 to 10, where:\n",
      "        - **10**: Highly relevant and correct, fully addressing the user's query without fabricating information.\n",
      "        - **1**: Completely irrelevant or incorrect, with significant issues in relevance or correctness.\n",
      "        \n",
      "        **Key Considerations for Rating:**\n",
      "        1. Relevance: How well does the agent's answer address the user's query based on the retrieved RAG information?\n",
      "        2. Consistency: Does the agent's answer align with the retrieved RAG information without introducing fabricated details?\n",
      "        3. Adherence to Facts: If the RAG information or reference answer lacks specific details, the agent must acknowledge it rather than make up information.\n",
      "        \n",
      "        **Important Notes:**\n",
      "        - Be fair and avoid petty deductions. If the agent answers correctly based on the RAG information and does not fabricate, it should receive a high score.\n",
      "        - If the agent correctly notes that the retrieved RAG information lacks sufficient details, this should not result in a lower score.\n",
      "        \n",
      "        **Response Format:**\n",
      "        Please respond with a single number between 1 and 10. Provide no additional text or explanation. Thank you.\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "query = 'What are the main topics covered in the NLP class?'\n",
    "agent_answer = \"The main topics covered in the NLP class may vary depending on the specific syllabus of the course, but typically include the following: 1. Introduction to NLP: Overview of NLP, its history, and applications. 2. Language Models: Basics of language models, such as n-gram models, n-gram smoothing, and hidden Markov models. 3. Text Preprocessing: Text cleaning, tokenization, stemming, lemmatization, and stop word removal. 4. Part-of-Speech Tagging: Rules-based and statistical methods for identifying parts of speech in a sentence. 5. Named Entity Recognition: Techniques for identifying and classifying named entities in text, such as people, locations, and organizations. 6. Sentiment Analysis: Methods for determining the sentiment or opinion expressed in a text. 7. Machine Translation: Overview of machine translation, rule-based and statistical methods, and recent advancements in neural machine translation. 8. Information Retrieval: Introduction to information retrieval systems, relevance ranking, and indexing. 9. Text Classification: Techniques for classifying text into predefined categories, such as spam detection and sentiment classification. 10. Dialogue Systems: Introduction to dialogue systems, including rule-based and statistical approaches. 11. Question Answering: Methods for building question-answering systems, including information retrieval-based and retrieval-based approaches. 12. Summarization: Overview of summarization techniques, such as extractive and abstractive summarization. 13. Text Generation: Introduction to text generation techniques, including rule-based and statistical methods. 14. NLP Applications: Case studies and applications of NLP in various domains, such as healthcare, finance, and customer service. These topics may be covered in different ways and with varying depth, depending on the specific goals and focus of the NLP course.\"\n",
    "reference_answer = \"The main topics covered in this course include Natural Language Processing (NLP) with a focus on language models. It covers both fundamental and advanced NLP topics, large-scale language models, real-world implications such as ethics, classical NLP practices, modern approaches, and hands-on experience in training and evaluating language models.\"\n",
    "rag_section = \"Course Description:\\nThe course delves into Natural Language Processing (NLP), emphasizing Language Models. It covers fundamentals and advanced NLP topics, including large-scale language models and their real-world implications, such as ethics. Students will develop skills in classical and modern NLP practices, along with hands-on experience in training and evaluating language models.\"\n",
    "\n",
    "# prompt = (\n",
    "#         \"Evaluate the relevance and correctness of the answers generated by our agent that reads the information retrieved by RAG. And we will provide you the following information:\\n\\n\"\n",
    "\n",
    "#         \"The user's query:\\n{query}\\n\\n\"\n",
    "#         \"Information retrieved by the RAG model:\\n{rag_section}\\n\\n\"\n",
    "#         \"Our agent's answer:\\n{agent_answer}\\n\\n\"\n",
    "#         \"The reference answer generated by human:\\n{reference_answer}\\n\\n\"\n",
    "\n",
    "#         \"Please base your rating on the following three points:\\n\"\n",
    "#         \"1. How relevant is the agent's answer to the user's query, given the information retrieved by the RAG?\\n\"\n",
    "#         \"2. Was the agent's answer consistent with the information retrieved and did the agent not fabricate any information?\\n\"\n",
    "#         \"3. Making things up is not allowed. According to the retrieved information from RAG and the reference answer, if you find out our agent is making up something, you should note that which is forbidden.\\n\\n\"\n",
    "\n",
    "#         \"It's important to note that don't be petty when scoring. As long as our agent correctly answers the user's query and doesn't make up anything, or correctly mentions that there are no such information in RAG's result, Then you should give it a high score.\\n\\n\"\n",
    "\n",
    "#         \"Please only return your rating result, which is a score between 1 and 10, where 10 indicates the highest relevance and correctness, and 1 indicates no relevance or correctness.\\n\\n\"\n",
    "\n",
    "#         \"Again, please only return a number, and nothing else. Thank you.\").format(query=query, rag_section=rag_section, agent_answer=agent_answer, reference_answer=reference_answer)\n",
    "\n",
    "prompt =\"\"\"\n",
    "        Evaluate the relevance and correctness of the answers generated by our agent based on the information retrieved by the RAG model. The evaluation will consider the following details:\n",
    "\n",
    "        1. User's Query:\n",
    "        {query}\n",
    "        \n",
    "        2. Information retrieved by the RAG model:\n",
    "        {rag_section}\n",
    "        \n",
    "        3. Agent's Answer:\n",
    "        {agent_answer}\n",
    "        \n",
    "        4. Reference Answer (generated by humans):\n",
    "        {reference_answer}\n",
    "        \n",
    "        **Rating Guidelines:**\n",
    "        \n",
    "        Rate the agent's answer on a scale of 1 to 10, where:\n",
    "        - **10**: Highly relevant and correct, fully addressing the user's query without fabricating information.\n",
    "        - **1**: Completely irrelevant or incorrect, with significant issues in relevance or correctness.\n",
    "        \n",
    "        **Key Considerations for Rating:**\n",
    "        1. Relevance: How well does the agent's answer address the user's query based on the retrieved RAG information?\n",
    "        2. Consistency: Does the agent's answer align with the retrieved RAG information without introducing fabricated details?\n",
    "        3. Adherence to Facts: If the RAG information or reference answer lacks specific details, the agent must acknowledge it rather than make up information.\n",
    "        \n",
    "        **Important Notes:**\n",
    "        - Be fair and avoid petty deductions. If the agent answers correctly based on the RAG information and does not fabricate, it should receive a high score.\n",
    "        - If the agent correctly notes that the retrieved RAG information lacks sufficient details, this should not result in a lower score.\n",
    "        \n",
    "        **Response Format:**\n",
    "        Please respond with a single number between 1 and 10. Provide no additional text or explanation. Thank you.\n",
    "        \"\"\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "4Knaf9ztEmkC"
   },
   "outputs": [],
   "source": [
    "def match_re(text):\n",
    "        matches = re.findall(r'\\b\\d+(?:,\\d{3})*(?:\\.\\d+)?\\b', text)\n",
    "        if matches:\n",
    "            return matches[-1]\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "def eval(query, agent_answer, reference_answer, rag_section, prompt=prompt):\n",
    "    prompt = prompt.format(query=query, rag_section=rag_section, agent_answer=agent_answer, reference_answer=reference_answer)\n",
    "    # print(prompt)\n",
    "    # print(len(prompt))\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a fair evaluator following specific guidelines.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.0,        # Deterministic output for fairness\n",
    "        top_p=1.0,              # Allow full consideration of relevant tokens\n",
    "        max_tokens=50,          # Limit the response to a single number\n",
    "    )\n",
    "\n",
    "    score = response.choices[0].message.content.strip()\n",
    "\n",
    "    return match_re(score), response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_RHennGRHmax",
    "outputId": "4690384e-e107-4fb8-ddb4-cdf0048ecc05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 7\n"
     ]
    }
   ],
   "source": [
    "score, reason = eval(query, agent_answer, reference_answer, rag_section)\n",
    "print(score, reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "fA5qRFW0H_V3"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def gpt_rate_answers(read_path, write_path):\n",
    "    content = pd.read_csv(read_path)\n",
    "    \n",
    "    queries = content['query'].tolist()\n",
    "    agent_answers = content['Agent_responds'].tolist()\n",
    "    reference_answers = content['reference_answer'].tolist()\n",
    "    rag_sections = content['RAG_retrieval_results'].tolist()\n",
    "\n",
    "    scores = []\n",
    "    for i in tqdm(range(len(queries))):\n",
    "      score, reason = eval(queries[i], agent_answers[i], reference_answers[i], rag_sections[i])\n",
    "      scores.append(score)\n",
    "      reasons.append(reason)\n",
    "    content['score'] = scores\n",
    "\n",
    "    content.to_csv(write_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_28k6HjIel7",
    "outputId": "e79c3073-0883-4e47-8a4c-07c4aea9bfb3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 305/305 [02:48<00:00,  1.81it/s]\n"
     ]
    }
   ],
   "source": [
    "gpt_rate_answers('./Qwen_responds.csv', './Qwen_ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 9.081967213114755\n"
     ]
    }
   ],
   "source": [
    "compute_average_score(\"./Qwen_ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Uhd4Ynu9iWr",
    "outputId": "298cd8c7-0ba9-4941-80e1-ac794e4bbfe4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 305/305 [02:34<00:00,  1.97it/s]\n"
     ]
    }
   ],
   "source": [
    "gpt_rate_answers('./GPT_responds.csv', './GPT_ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 9.39016393442623\n"
     ]
    }
   ],
   "source": [
    "compute_average_score(\"./GPT_ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jScH_FOo_Z5L",
    "outputId": "2dc05e0c-813a-4443-f679-2cb159f1ad28"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 305/305 [02:22<00:00,  2.13it/s]\n"
     ]
    }
   ],
   "source": [
    "gpt_rate_answers('./Llama_responds.csv', './Llama_ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 9.160655737704918\n"
     ]
    }
   ],
   "source": [
    "compute_average_score(\"./Llama_ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 305/305 [02:38<00:00,  1.92it/s]\n"
     ]
    }
   ],
   "source": [
    "gpt_rate_answers('./GLM_responds.csv', './GLM_ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 9.039344262295081\n"
     ]
    }
   ],
   "source": [
    "compute_average_score(\"./GLM_ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S4vqchGlCm_G",
    "outputId": "63baadc9-ba86-427a-90f1-b6bac21cbb88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 9.081967213114755\n",
      "Mean: 9.39016393442623\n",
      "Mean: 9.160655737704918\n",
      "Mean: 9.039344262295081\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compute_average_score(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"score\"] = pd.to_numeric(df[\"score\"])\n",
    "\n",
    "    mean_value = df[\"score\"].mean()\n",
    "    print(\"Mean:\", mean_value)\n",
    "\n",
    "compute_average_score(\"./Qwen_ratings.csv\")\n",
    "compute_average_score(\"./GPT_ratings.csv\")\n",
    "compute_average_score(\"./Llama_ratings.csv\")\n",
    "compute_average_score(\"./GLM_ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCMl0egyRWdW"
   },
   "outputs": [],
   "source": [
    "# 1. GPT 2. LLaMA 3. Qwen 4. GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
